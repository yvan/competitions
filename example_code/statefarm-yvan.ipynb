{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "from utils_yvan import save_array, load_array\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.regularizers import l2\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = os.path.join('/scratch', 'yns207', 'data_statefarm')\n",
    "path = DATA_DIR\n",
    "test_path = os.path.join(path, 'test')\n",
    "models_path = os.path.join(path, 'results')\n",
    "train_path = os.path.join(path, 'train')\n",
    "valid_path = os.path.join(path, 'valid')\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "tr_batches = gen.flow_from_directory(train_path, target_size=(224,224), class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "va_batches = gen.flow_from_directory(valid_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size*2)\n",
    "te_batches = gen.flow_from_directory(test_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "va_classes = va_batches.classes\n",
    "tr_classes = tr_batches.classes\n",
    "va_labels = to_categorical(va_classes)\n",
    "tr_labels = to_categorical(tr_classes)\n",
    "va_filenames = va_batches.filenames\n",
    "tr_filenames = tr_batches.filenames\n",
    "te_filenames = te_batches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(tr_batches, \n",
    "                    steps_per_epoch=tr_batches.n//batch_size, \n",
    "                    validation_data=va_batches, \n",
    "                    validation_steps=va_batches.n//batch_size,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jump straight ahead to the single conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    hist1 = model.fit_generator(tr_batches, \n",
    "                    steps_per_epoch=(tr_batches.n//batch_size)+1, \n",
    "                    validation_data=va_batches, \n",
    "                    validation_steps=(va_batches.n//batch_size)+1,\n",
    "                    epochs=2)\n",
    "    model.optimizer.lr = 0.001\n",
    "    hist2 = model.fit_generator(tr_batches, \n",
    "                        steps_per_epoch=(tr_batches.n//batch_size)+1, \n",
    "                        validation_data=va_batches, \n",
    "                        validation_steps=(va_batches.n//batch_size)+1,\n",
    "                        epochs=4)\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "tr_batches = gen_t.flow_from_directory(train_path, target_size=(224,224), class_mode='categorical', shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(tr_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.tr = 0.0001\n",
    "hist = model.fit_generator(tr_batches, \n",
    "                        steps_per_epoch=(tr_batches.n//batch_size)+1, \n",
    "                        validation_data=va_batches, \n",
    "                        validation_steps=(va_batches.n//batch_size)+1,\n",
    "                        epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Convolution2D(32,(3,3),activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3),activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(tr_batches, \n",
    "                        steps_per_epoch=(tr_batches.n//batch_size)+1, \n",
    "                        validation_data=va_batches, \n",
    "                        validation_steps=(va_batches.n//batch_size)+1,\n",
    "                        epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "hist = model.fit_generator(tr_batches, \n",
    "                        steps_per_epoch=(tr_batches.n//batch_size)+1, \n",
    "                        validation_data=va_batches, \n",
    "                        validation_steps=(va_batches.n//batch_size)+1,\n",
    "                        epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "hist = model.fit_generator(tr_batches, \n",
    "                        steps_per_epoch=(tr_batches.n//batch_size)+1, \n",
    "                        validation_data=va_batches, \n",
    "                        validation_steps=(va_batches.n//batch_size)+1,\n",
    "                        epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try vgg conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "last_conv_index = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_index+1]\n",
    "fc_layers = model.layers[last_conv_index+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "When you fit your conv features, conv_feat was created from shuffled batches, \n",
    "but trn_labels was not shuffled, so they don't match. \n",
    "You need to not shuffle the batches used to create conv_feat.\n",
    "'''\n",
    "gen = image.ImageDataGenerator()\n",
    "\n",
    "tr_batches = gen.flow_from_directory(train_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "va_batches = gen.flow_from_directory(valid_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size*2)\n",
    "te_batches = gen.flow_from_directory(test_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "va_classes = va_batches.classes\n",
    "tr_classes = tr_batches.classes\n",
    "va_labels = to_categorical(va_classes)\n",
    "tr_labels = to_categorical(tr_classes)\n",
    "va_filenames = va_batches.filenames\n",
    "tr_filenames = tr_batches.filenames\n",
    "te_filenames = te_batches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_tr_feat = conv_model.predict_generator(tr_batches, (tr_batches.n//batch_size)+1, workers=2)\n",
    "save_array(os.path.join(models_path, 'conv_tr_feat.dat'), conv_tr_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide by 2 because the validation batch size is 2x bigger\n",
    "conv_va_feat = conv_model.predict_generator(va_batches, (va_batches.n//(batch_size*2))+1, workers=2)\n",
    "save_array(os.path.join(models_path, 'conv_va_feat.dat'), conv_va_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_tr_path = os.path.join(models_path, 'conv_tr_feat.dat')\n",
    "conv_tr_feat = load_array(conv_tr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_va_path = os.path.join(models_path, 'conv_va_feat.dat')\n",
    "conv_va_feat = load_array(conv_hva_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batchnorm dense layers on pretrained conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "http://forums.fast.ai/t/statefarm-kaggle-comp/183/124\n",
    "details on a dif architecture to improve performance\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit(conv_tr_feat,\n",
    "            tr_labels,\n",
    "            batch_size=batch_size,\n",
    "            epochs=10,\n",
    "            validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit(conv_tr_feat,\n",
    "            tr_labels,\n",
    "            batch_size=batch_size,\n",
    "            epochs=10,\n",
    "            validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "aug_batches = gen_t.flow_from_directory(train_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create one tahts 2x bigger than original training\n",
    "# the idea is that there will be an even distribution\n",
    "# of the ranomd sutff in there.\n",
    "aug_conv_feat = conv_model.predict_generator(aug_batches, (2*tr_batches.n//batch_size)+1, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(os.path.join(models_path, 'aug_conv_feat.dat'), aug_conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_conv_feat = load_array(os.path.join(models_path, 'aug_conv_feat.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this crashes the kernel as we run out of RAM\n",
    "#need to use batches instead,\n",
    "#apparently jeremy shows an example somewhere\n",
    "# could also just NOT precompute the conv features, run\n",
    "# a normal training round.\n",
    "#aug_conv_feat_c = np.concatenate([aug_conv_feat, conv_tr_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_conv_labels = np.concatenate([tr_labels]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_da_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit(aug_conv_feat,\n",
    "             aug_conv_labels,\n",
    "             batch_size=batch_size,\n",
    "             epochs=4,\n",
    "             validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit(aug_conv_feat,\n",
    "             aug_conv_labels,\n",
    "             batch_size=batch_size,\n",
    "             epochs=4,\n",
    "             validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit(aug_conv_feat,\n",
    "             aug_conv_labels,\n",
    "             batch_size=batch_size,\n",
    "             epochs=4,\n",
    "             validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retry of batch_norm, data augmentation, and psuedo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ok scratch all the pretrained BS, we need to\n",
    "# generate batches with the mixiterator\n",
    "# that contain 1/4 psuedo labelled data\n",
    "# and 3/4 training data and then pass this \n",
    "# through the whole model conv + fc layers\n",
    "# actually do not need to use the mixiterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
      "/home/yns207/nbs/machine_learning/vgg16.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "  model.add(Convolution2D(filters, 3, 3, activation='relu'))\n"
     ]
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "last_conv_index = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_index+1]\n",
    "fc_layers = model.layers[last_conv_index+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18131 images belonging to 10 classes.\n",
      "Found 18131 images belonging to 10 classes.\n",
      "Found 4293 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "gen_t = image.ImageDataGenerator(rotation_range=15, \n",
    "                                height_shift_range=0.05, \n",
    "                                shear_range=0.1, \n",
    "                                channel_shift_range=20, \n",
    "                                width_shift_range=0.1)\n",
    "\n",
    "tr_batches = gen_t.flow_from_directory(train_path, target_size=(224,224), class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "tr_batches_fixed = gen.flow_from_directory(train_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "va_batches = gen.flow_from_directory(valid_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size*2)\n",
    "te_batches = gen.flow_from_directory(test_path, target_size=(224,224), class_mode='categorical', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "va_classes = va_batches.classes\n",
    "tr_classes = tr_batches_fixed.classes\n",
    "va_labels = to_categorical(va_classes)\n",
    "tr_labels = to_categorical(tr_classes)\n",
    "va_filenames = va_batches.filenames\n",
    "tr_filenames = tr_batches.filenames\n",
    "te_filenames = te_batches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvan_model = Sequential()\n",
    "conv_model = Sequential(conv_layers)\n",
    "for l in conv_model.layers:\n",
    "    l.trainable = False\n",
    "yvan_model.add(conv_model)\n",
    "yvan_model.add(Sequential(get_bn_da_layers(p)))\n",
    "adam = Adam(lr=0.001)\n",
    "yvan_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_5 (Sequential)    (None, 512, 14, 14)       14714688  \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 10)                5061610   \n",
      "=================================================================\n",
      "Total params: 19,776,298\n",
      "Trainable params: 5,060,810\n",
      "Non-trainable params: 14,715,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "yvan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "284/284 [==============================] - 847s - loss: 1.9788 - acc: 0.4798 - val_loss: 0.6760 - val_acc: 0.8053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af4dfc52128>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yvan_model.fit_generator(tr_batches,\n",
    "                         steps_per_epoch=(tr_batches.n//batch_size)+1,\n",
    "                         validation_data=va_batches,\n",
    "                         validation_steps=(va_batches.n//(batch_size*2))+1,\n",
    "                         epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "284/284 [==============================] - 985s - loss: 0.5017 - acc: 0.8362 - val_loss: 0.6218 - val_acc: 0.8216\n",
      "Epoch 2/4\n",
      "284/284 [==============================] - 1225s - loss: 0.3086 - acc: 0.9032 - val_loss: 0.6487 - val_acc: 0.8027\n",
      "Epoch 3/4\n",
      "284/284 [==============================] - 1284s - loss: 0.2481 - acc: 0.9204 - val_loss: 0.6532 - val_acc: 0.8169\n",
      "Epoch 4/4\n",
      "234/284 [=======================>......] - ETA: 198s - loss: 0.1992 - acc: 0.9374"
     ]
    }
   ],
   "source": [
    "hist = yvan_model.fit_generator(tr_batches,\n",
    "                         steps_per_epoch=(tr_batches.n//batch_size)+1,\n",
    "                         validation_data=va_batches,\n",
    "                         validation_steps=(va_batches.n//(batch_size*2))+1,\n",
    "                         epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.83602669459500178,\n",
       "  0.90298383983233133,\n",
       "  0.92046770722530347,\n",
       "  0.93910981190789133],\n",
       " 'loss': [0.50245500503852825,\n",
       "  0.30926036406321766,\n",
       "  0.24712512025790526,\n",
       "  0.19637700660066593],\n",
       " 'val_acc': [0.82156999761161897,\n",
       "  0.80270207293406248,\n",
       "  0.81691125088739913,\n",
       "  0.82366643388899452],\n",
       " 'val_loss': [0.62175132371372421,\n",
       "  0.64871419478540182,\n",
       "  0.65319442587967325,\n",
       "  0.64364949728874454]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then save those weights\n",
    "yvan_model.save_weights(os.path.join(models_path, 'yvan_model_weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvan_model.load_weights(os.path.join(models_path, 'yvan_model_weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then use that training to pseudo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pseudo_labels = yvan_model.predict_generator(va_batches, steps=va_batches.n//(batch_size*2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pseudo_labels = (val_pseudo_labels == val_pseudo_labels.max(axis=1, keepdims=1)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pseudo_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseudo_labels = np.concatenate([tr_labels, val_pseudo_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zeroth elem because thats the actual data\n",
    "#it actually returns a tuple with something else in it\n",
    "va_pseudo_data = np.concatenate([va_batches.next()[0] for i in range((va_batches.n//(batch_size*2))+1)])\n",
    "tr_fixed_data = np.concatenate([tr_batches_fixed.next()[0] for i in range((tr_batches_fixed.n//batch_size)+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(os.path.join(models_path, 'va_pseudo_data.dat'), va_pseudo_data)\n",
    "save_array(os.path.join(models_path, 'tr_fixed_data.dat'), tr_fixed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseudo_data = np.concatenate([tr_fixed_data, va_pseudo_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22424, 3, 224, 224)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4293, 3, 224, 224)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_pseudo_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4293, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then train on the pseudo labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22424 samples, validate on 4293 samples\n",
      "Epoch 1/4\n",
      " 1088/22424 [>.............................] - ETA: 952s - loss: 1.3552 - acc: 0.7895"
     ]
    }
   ],
   "source": [
    "hist = yvan_model.fit(pseudo_data,\n",
    "                    pseudo_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(va_pseudo_data, va_labels),\n",
    "                    epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.86327149484823573,\n",
       "  0.90385301462718515,\n",
       "  0.91772208348198359,\n",
       "  0.92204780590096158],\n",
       " 'loss': [0.52636034737261028,\n",
       "  0.34129503553270063,\n",
       "  0.28946414123124153,\n",
       "  0.27423459440310255],\n",
       " 'val_acc': [0.78593058467272303,\n",
       "  0.79431632891099491,\n",
       "  0.79734451432564635,\n",
       "  0.80246913580246915],\n",
       " 'val_loss': [0.6578886400089935,\n",
       "  0.58394270200927079,\n",
       "  0.56764786423377267,\n",
       "  0.55063074598105777]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22424 samples, validate on 4293 samples\n",
      "Epoch 1/5\n",
      "22424/22424 [==============================] - 1685s - loss: 0.2613 - acc: 0.9269 - val_loss: 0.5417 - val_acc: 0.8064\n",
      "Epoch 2/5\n",
      "22400/22424 [============================>.] - ETA: 1s - loss: 0.2489 - acc: 0.9309"
     ]
    }
   ],
   "source": [
    "yvan_model.optimizer.lr = 0.0001\n",
    "hist = yvan_model.fit(pseudo_data,\n",
    "                    pseudo_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(va_pseudo_data, va_labels),\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.92686407422747219,\n",
       "  0.93092222616496445,\n",
       "  0.92998572957545489,\n",
       "  0.93168034251145371,\n",
       "  0.93444523724580808],\n",
       " 'loss': [0.26126723269501034,\n",
       "  0.24872710621712082,\n",
       "  0.23935677077524201,\n",
       "  0.23384266970011322,\n",
       "  0.2264022688715959],\n",
       " 'val_acc': [0.80642907058348501,\n",
       "  0.78965758211388337,\n",
       "  0.80526438388073607,\n",
       "  0.79757745166897298,\n",
       "  0.80130444910319121],\n",
       " 'val_loss': [0.54172315662511994,\n",
       "  0.55974224888026058,\n",
       "  0.55361377409835699,\n",
       "  0.5439553114186626,\n",
       "  0.52685085808150622]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvan_model.save_weights(os.path.join(models_path, 'yvan_model_weights2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvan_model.load_weights(os.path.join(models_path, 'yvan_model_weights2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then use it to make predictions on the test set\n",
    "#then normalize test set, submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = yvan_model.predict_generator(te_batches, steps=(te_batches.n//batch_size)+1, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79726, 10)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(os.path.join(models_path, 'predictions_f.dat'), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crop thes predictions to prevent overconfident false predictions\n",
    "# which are punished excessively for multiclass loss\n",
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds, 0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img_31304.jpg',\n",
       " 'img_56510.jpg',\n",
       " 'img_36143.jpg',\n",
       " 'img_95819.jpg',\n",
       " 'img_1960.jpg']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get file names\n",
    "from glob import glob\n",
    "g = glob(os.path.join(test_path, 'uknown/*'))\n",
    "img_names = [f[43:] for f in g]\n",
    "img_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_path = os.path.join(models_path, 'sub_may23_2017.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sub = pd.DataFrame(subm, columns=['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9'])\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "sub['img'] = pd.Series(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01249</td>\n",
       "      <td>0.62044</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.18909</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.14855</td>\n",
       "      <td>0.01504</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>img_31304.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.81733</td>\n",
       "      <td>0.16374</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.01078</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>img_56510.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02991</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.01529</td>\n",
       "      <td>0.01115</td>\n",
       "      <td>0.01016</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.87228</td>\n",
       "      <td>0.04765</td>\n",
       "      <td>img_36143.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.01056</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.93000</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>img_95819.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00912</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.46916</td>\n",
       "      <td>0.33204</td>\n",
       "      <td>0.02055</td>\n",
       "      <td>0.02083</td>\n",
       "      <td>0.12988</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>img_1960.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       c0      c1      c2      c3      c4      c5      c6      c7      c8  \\\n",
       "0 0.01249 0.62044 0.00778 0.18909 0.00778 0.00778 0.14855 0.01504 0.00778   \n",
       "1 0.00778 0.00778 0.00778 0.81733 0.16374 0.00778 0.00778 0.00778 0.01078   \n",
       "2 0.02991 0.00778 0.00778 0.01529 0.01115 0.01016 0.00778 0.00778 0.87228   \n",
       "3 0.00778 0.01056 0.00778 0.00778 0.00778 0.00778 0.00778 0.93000 0.00778   \n",
       "4 0.00912 0.00778 0.00778 0.00778 0.46916 0.33204 0.02055 0.02083 0.12988   \n",
       "\n",
       "       c9            img  \n",
       "0 0.00778  img_31304.jpg  \n",
       "1 0.00778  img_56510.jpg  \n",
       "2 0.04765  img_36143.jpg  \n",
       "3 0.00778  img_95819.jpg  \n",
       "4 0.00778   img_1960.jpg  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(sub_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/scratch/yns207/data_statefarm/results/sub_may23_2017.gz' target='_blank'>/scratch/yns207/data_statefarm/results/sub_may23_2017.gz</a><br>"
      ],
      "text/plain": [
       "/scratch/yns207/data_statefarm/results/sub_may23_2017.gz"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(sub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# the crap below didnt work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bn_model.save_weights(os.path.join(models_path, 'aug_conv_weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "val_pseudo_labels = bn_model.predict(conv_va_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pseudo_labels = np.concatenate([tr_labels, val_pseudo_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pseudo_feat = np.concatenate([conv_tr_feat, conv_va_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bn_model.load_weights(os.path.join(models_path, 'conv_weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bn_model.fit(pseudo_feat,\n",
    "             pseudo_labels,\n",
    "             batch_size=batch_size,\n",
    "             epochs=5,\n",
    "             validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bn_model.optimizer.lr=0.001\n",
    "bn_model.fit(pseudo_feat,\n",
    "             pseudo_labels,\n",
    "             batch_size=batch_size,\n",
    "             epochs=5,\n",
    "             validation_data=(conv_va_feat,va_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bn_model.save_weights(os.path.join(models_path, 'conv_weights_f.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "va_preds = bn_model.predict(conv_va_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cat_entropy_va = categorical_crossentropy(va_labels, do_clip(va_preds, 0.93)).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cat_entropy_va.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "conv_te_feat = conv_model.predict_generator(te_batches, (te_batches.n//batch_size)+1)\n",
    "save_array(os.path.join(models_path, 'conv_te_feat.dat'), conv_te_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "conv_te_feat = load_array(os.path.join(models_path, 'conv_te_feat.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "te_preds = bn_model.predict(conv_te_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sub = do_clip(te_peds, 0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sub_path = os.path.join(models_path, 'sub_may172017.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "classes = sorted(tr_batches.class_indices, key=batches.class_indices.get)\n",
    "subm = pd.DataFrame(sub, columns=classes)\n",
    "subm.insert(0, 'img', [a[4:] for a in test_filenames])\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "subm.to_csv(sub_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "FileLink(sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
