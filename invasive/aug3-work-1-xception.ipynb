{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-636e8ca866bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yns207/anaconda3/envs/keras-py3/lib/python3.5/site-packages/tensorflow/python/client/device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yns207/anaconda3/envs/keras-py3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mlist_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mListDevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yns207/anaconda3/envs/keras-py3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yns207/anaconda3/envs/keras-py3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, glob, bcolz\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_input_incep_xcep\n",
    "from keras.applications.imagenet_utils import preprocess_input as preprocess_input_vgg_resnet\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_img(img_path, img_shape):\n",
    "    img = misc.imread(img_path)\n",
    "    img = misc.imresize(img, img_shape)\n",
    "    return img\n",
    "\n",
    "def read_imgs(img_height, img_width):\n",
    "    train_img, test_img = [],[]\n",
    "    for img_path in tqdm(train_set['name'].iloc[:]):\n",
    "        train_img.append(read_img(os.path.join(path, 'train', str(img_path)+'.jpg'), (img_height, img_width)))\n",
    "\n",
    "    for img_path in tqdm(test_set['name'].iloc[:]):\n",
    "        test_img.append(read_img(os.path.join(path, 'test', str(img_path)+'.jpg'), (img_height, img_width)))\n",
    "    return np.array(train_img), np.array(test_img)\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def freeze_model(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "def grab_optimizer(opt, lr):\n",
    "    if opt == 'sgd':\n",
    "        return optimizers.SGD(lr=lr, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    elif opt == 'adam':\n",
    "        return optimizers.Adam(lr=lr)\n",
    "    elif opt == 'adagrad':\n",
    "        return optimizers.Adagrad(lr=lr)\n",
    "    elif opt == 'rmsprop':\n",
    "        return optimizers.RMSprop(lr=lr)\n",
    "    \n",
    "def dense_block(units, activation, drop_prob, inputs):\n",
    "    x = Dense(units, activation=None)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    return x\n",
    "\n",
    "def make_conv_model(input_shape, optimizer):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    m = conv_block(16, (3,3), (2,2),'relu', inputs=inputs)\n",
    "    m = conv_block(32, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = conv_block(64, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = conv_block(128, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = conv_block(256, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = Flatten()(m)\n",
    "    m = dense_block(2048, 'relu', 0.25, inputs=m)\n",
    "    m = dense_block(512, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_vgg19_ft(input_shape, optimizer):\n",
    "    base_model = VGG19(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "    base_model = freeze_model(base_model)\n",
    "    m = Flatten()(base_model.layers[-1].output)\n",
    "    m = dense_block(1024, 'relu', 0.25, inputs=m)\n",
    "    m = dense_block(1024, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_incepv3_ft(input_shape, optimizer):\n",
    "    base_model = InceptionV3(input_shape=input_shape, weights='imagenet', include_top=False, pooling='avg')\n",
    "    base_model = freeze_model(base_model)\n",
    "    m = dense_block(1024, 'relu', 0.25, inputs=base_model.layers[-1].output)\n",
    "    m = dense_block(1024, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_resnet50_ft(input_shape, optimizer):\n",
    "    base_model = ResNet50(input_shape=input_shape, weights='imagenet', include_top=False, pooling='avg')\n",
    "    base_model = freeze_model(base_model)\n",
    "    m = dense_block(1024, 'relu', 0.25, inputs=base_model.layers[-1].output)\n",
    "    m = dense_block(1024, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_xception_ft(input_shape, optimizer):\n",
    "    base_model = Xception(input_shape=input_shape, weights='imagenet', include_top=False, pooling='avg')\n",
    "    base_model = freeze_model(base_model)\n",
    "    m = dense_block(1024, 'relu', 0.25, inputs=base_model.layers[-1].output)\n",
    "    m = dense_block(1024, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# does not use precomputiation so it can use data augmentation\n",
    "def train_kfolds_ft_conv(model, train_data, train_label, gen_train, gen_valid, model_out, model_init_weights, epochs, kfolds, batch_size):\n",
    "    kf = KFold(n_splits=kfolds, shuffle=True)\n",
    "    \n",
    "    i = 0\n",
    "    models_stats = {}\n",
    "    for train_ixs, valid_ixs in kf.split(train_data):\n",
    "        x_train = train_data[train_ixs]\n",
    "        x_valid = train_data[valid_ixs]\n",
    "        y_train = train_label[train_ixs]\n",
    "        y_valid = train_label[valid_ixs]\n",
    "                \n",
    "        #re-initialzie the weights of the model on each run\n",
    "        #by loading thi intiial stored weights from file\n",
    "        model.load_weights(model_init_weights)\n",
    "        model_out_file = '{}_{}.model'.format(model_out, str(i))\n",
    "        model_checkpoint = ModelCheckpoint(model_out_file, \n",
    "                                            monitor='val_loss', \n",
    "                                            save_best_only=True)\n",
    "        \n",
    "        # fit the dense layers for 1/4 the epochs\n",
    "        model.fit_generator(gen_train.flow(x_train, y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=(len(x_train)//batch_size)+1,\n",
    "                            validation_data=gen_valid.flow(x_valid, y_valid, batch_size=batch_size),\n",
    "                            validation_steps=(len(x_valid)//batch_size)+1,\n",
    "                            epochs=epochs//4,\n",
    "                            verbose=1,\n",
    "                            callbacks=[model_checkpoint])\n",
    "        print('training last 3 conv layers:')\n",
    "        # set last conv layer to trainable and fit for 3/4 the epochs\n",
    "        conv_layers = [layer for layer in model.layers if type(layer) is Convolution2D]\n",
    "        for layer in conv_layers[-3:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        model.fit_generator(gen_train.flow(x_train, y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=(len(x_train)//batch_size)+1,\n",
    "                            validation_data=gen_valid.flow(x_valid, y_valid, batch_size=batch_size),\n",
    "                            validation_steps=(len(x_valid)//batch_size)+1,\n",
    "                            epochs=epochs-(epochs//4),\n",
    "                            verbose=1,\n",
    "                            callbacks=[model_checkpoint])\n",
    "        \n",
    "        model.load_weights(model_out_file)\n",
    "        \n",
    "        eval_tr = model.evaluate(x_train, y_train)\n",
    "        eval_va = model.evaluate(x_valid, y_valid)\n",
    "        \n",
    "        tr_score = roc_auc_score(y_train, model.predict(x_train)[:, 0])\n",
    "        va_score = roc_auc_score(y_valid, model.predict(x_valid)[:, 0])\n",
    "        \n",
    "        print('\\n')\n",
    "        print('kfold: {}'.format(str(i)))\n",
    "        print('best model train acc: {}, loss: {}'.format(eval_tr[1], eval_tr[0]))\n",
    "        print('best model valid acc: {}, loss: {}'.format(eval_va[1], eval_va[0]))\n",
    "        print('best model train aroc score: {}, valid aroc score: {}'.format(tr_score, va_score))\n",
    "        print('\\n')\n",
    "        models_stats[model_out_file] = {'score_tr_va':[tr_score, va_score], 'train_acc_loss':[eval_tr[1], eval_tr[0]], 'val_acc_loss':[eval_va[1], eval_va[0]]}\n",
    "        \n",
    "        with open(os.path.join(models_path,'{}_{}.out'.format(model_out,'history')), 'a') as f:\n",
    "            f.write('kfold: {}'.format(str(i)))\n",
    "            f.write('best model train acc: {}, loss: {}'.format(eval_tr[1], eval_tr[0]))\n",
    "            f.write('best model valid acc: {}, loss: {}'.format(eval_va[1], eval_va[0]))\n",
    "            f.write('best model train aroc score: {}, valid aroc score: {}'.format(tr_score, va_score))\n",
    "            f.write('\\n')\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return models_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "setup data dirs and read in imgs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: /scratch/yns207/data_invasive\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join('/scratch', 'yns207', 'data_invasive')\n",
    "\n",
    "path = DATA_DIR\n",
    "test_path = os.path.join(path, 'test')\n",
    "models_path = os.path.join(path, 'results')\n",
    "train_path = os.path.join(path, 'train')\n",
    "valid_path = os.path.join(path, 'valid')\n",
    "print('DATA_PATH: ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n",
    "test_set = pd.read_csv(os.path.join(path, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_img, test_img = read_imgs(300,400)\n",
    "train_label = np.array(train_set['invasive'].iloc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "create a holdout set of 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_img, hold_img, train_labels, hold_labels = train_test_split(train_img, train_label, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_img.shape, hold_img.shape, train_labels.shape, hold_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "save the datasets unaltered so they can be loaded again at a later point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%cd $models_path\n",
    "save_array('aug_3_train_img.dat', train_img)\n",
    "save_array('aug_3_hold_img.dat', hold_img)\n",
    "save_array('aug_3_train_labels.dat', train_labels)\n",
    "save_array('aug_3_hold_labels.dat', hold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "read the datasets with bcolz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/yns207/data_invasive/results\n"
     ]
    }
   ],
   "source": [
    "%cd $models_path\n",
    "train_img = load_array('aug_3_train_img.dat')\n",
    "hold_img = load_array('aug_3_hold_img.dat')\n",
    "train_labels = load_array('aug_3_train_labels.dat')\n",
    "hold_labels = load_array('aug_3_hold_labels.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2065, 300, 400, 3), (230, 300, 400, 3), (2065,), (230,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape, hold_img.shape, train_labels.shape, hold_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "kfolds = 5\n",
    "lr = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%cd $models_path\n",
    "model_name = 'invasive_xception_conv_aug3'\n",
    "init_weights_model = '{}_base.model'.format(model_name)\n",
    "\n",
    "# create model and save initial weights\n",
    "model = make_xception_ft(train_img[0].shape, grab_optimizer('adam', lr))\n",
    "model.save_weights(init_weights_model)\n",
    "\n",
    "#make generators\n",
    "gen_train = ImageDataGenerator(\n",
    "    rotation_range = 30,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    "    fill_mode = 'nearest')\n",
    "\n",
    "gen_valid = ImageDataGenerator()\n",
    "\n",
    "#demean data coming through generators\n",
    "gen_train.mean = np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
    "gen_valid.mean = np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
    "\n",
    "# train dense model on folds\n",
    "performance3 = train_kfolds_ft_conv(model, train_img, train_labels, gen_train, gen_valid, model_name, init_weights_model, epochs, kfolds, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
