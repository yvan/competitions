{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15870554199573971419, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11332668621\n",
       " locality {\n",
       "   bus_id: 2\n",
       " }\n",
       " incarnation: 17261517923572989447\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:84:00.0\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "import os, gc, sys, glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers import Input, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/yns207/data_invasive\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join('/scratch', 'yns207', 'data_invasive')\n",
    "\n",
    "path = DATA_DIR\n",
    "test_path = os.path.join(path, 'test')\n",
    "models_path = os.path.join(path, 'results')\n",
    "train_path = os.path.join(path, 'train')\n",
    "valid_path = os.path.join(path, 'valid')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/yns207/data_invasive\n",
      "\n",
      "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=fr_FR.UTF-8,Utf16=on,HugeFiles=on,64 bits,28 CPUs Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz (406F1),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        2 files, 3353865207 bytes (3199 MiB)\n",
      "    \n",
      "Extracting archive: test.7z\n",
      "--\n",
      "Path = test.7z\n",
      "Type = 7z\n",
      "Physical Size = 1227586386\n",
      "Headers Size = 16731\n",
      "Method = LZMA2:24\n",
      "Solid = +\n",
      "Blocks = 1\n",
      "\n",
      "      0% 1 - test/1.jp                    0% 7 - test/1003.j                      0% 17 - test/1012.jp                        0% 23 - test/1018.jp                        0% 32 - test/1026.jp                        0% 37 - test/1030.jp                        1% 44 - test/1037.jp                        1% 52 - test/1044.jp                        1% 63 - test/1054.jp                        1% 68 - test/1059.jp                        1% 73 - test/1063.jp                        1% 78 - test/1068.jp                        2% 88 - test/1077.jp                        2% 93 - test/1081.jp                        2% 97 - test/1085.jp                        2% 101 - test/1089.j                        2% 106 - test/1093.j                        2% 117 - test/1102.j                        2% 122 - test/1107.j                        3% 127 - test/1111.j                        3% 137 - test/1120.j                        3% 142 - test/1125.j                        3% 148 - test/1130.j                        3% 158 - test/114.jp                        3% 163 - test/1144.j                        3% 168 - test/1149.j                        4% 173 - test/1153.j                        4% 184 - test/1163.j                        4% 189 - test/1168.j                        4% 192 - test/1170.j                        4% 197 - test/1175.j                        4% 203 - test/1180.j                        4% 213 - test/119.jp                        5% 219 - test/1195.j                        5% 223 - test/1199.j                        5% 228 - test/1202.j                        5% 238 - test/1211.j                        5% 242 - test/1215.j                        5% 247 - test/122.jp                        5% 252 - test/1224.j                        6% 257 - test/1229.j                        6% 266 - test/1237.j                        6% 272 - test/1242.j                        6% 276 - test/1246.j                        6% 282 - test/1251.j                        6% 291 - test/126.jp                        6% 295 - test/1263.j                        7% 300 - test/1268.j                        7% 306 - test/1273.j                        7% 315 - test/1281.j                        7% 321 - test/1287.j                        7% 326 - test/1291.j                        7% 331 - test/1296.j                        7% 336 - test/130.jp                        8% 340 - test/1303.j                        8% 344 - test/1307.j                        8% 349 - test/1311.j                        8% 359 - test/1320.j                        8% 364 - test/1325.j                        8% 375 - test/1335.j                        8% 380 - test/134.jp                        9% 385 - test/1344.j                        9% 394 - test/1352.j                        9% 400 - test/1358.j                        9% 404 - test/1361.j                        9% 408 - test/1365.j                        9% 415 - test/1371.j                        9% 419 - test/1375.j                       10% 426 - test/1381.j                       10% 431 - test/1386.j                       10% 436 - test/1390.j                       10% 440 - test/1394.j                       10% 445 - test/1399.j                       10% 446 - test/14.j                     10% 450 - test/1402.j                       10% 454 - test/1406.j                       11% 466 - test/1417.j                       11% 474 - test/1424.j                       11% 479 - test/1429.j                       11% 484 - test/1433.j                       11% 493 - test/1441.j                       11% 501 - test/1449.j                       12% 505 - test/1452.j                       12% 511 - test/1458.j                       12% 515 - test/1461.j                       12% 524 - test/147.jp                       12% 525 - test/1470.j                       12% 534 - test/1479.j                       12% 539 - test/1483.j                       13% 543 - test/1487.j                       13% 548 - test/1491.j                       13% 557 - test/15.j                     13% 562 - test/1503.j                       13% 567 - test/1508.j                       13% 572 - test/1512.j                       13% 578 - test/1518.j                       13% 582 - test/1521.j                       14% 585 - test/1524.j                       14% 590 - test/1529.j                       14% 595 - test/155.jp                       14% 599 - test/159.jp                       14% 605 - test/164.jp                       14% 613 - test/171.jp                       14% 617 - test/175.jp                       14% 623 - test/180.jp                       15% 628 - test/185.jp                       15% 632 - test/189.jp                       15% 637 - test/193.jp                       15% 646 - test/200.jp                       15% 652 - test/206.jp                       15% 657 - test/210.jp                       15% 662 - test/215.jp                       16% 673 - test/225.jp                       16% 678 - test/23.j                     16% 684 - test/235.jp                       16% 689 - test/24.j                     16% 693 - test/243.jp                       16% 698 - test/248.jp                       16% 702 - test/251.jp                       16% 706 - test/255.jp                       17% 711 - test/26.j                     17% 716 - test/264.jp                       17% 725 - test/272.jp                       17% 730 - test/277.jp                       17% 734 - test/280.jp                       17% 740 - test/286.jp                       18% 750 - test/295.jp                       18% 754 - test/299.jp                       18% 761 - test/304.jp                       18% 765 - test/308.jp                       18% 772 - test/314.jp                       18% 778 - test/32.j                     18% 781 - test/322.jp                       18% 789 - test/33.j                     19% 795 - test/335.jp                       19% 799 - test/339.jp                       19% 804 - test/343.jp                       19% 815 - test/353.jp                       19% 820 - test/358.jp                       19% 824 - test/361.jp                       19% 828 - test/365.jp                       20% 833 - test/37.j                     20% 838 - test/374.jp                       20% 842 - test/378.jp                       20% 853 - test/388.jp                       20% 854 - test/389.jp                       20% 863 - test/397.jp                       20% 868 - test/400.jp                       20% 872 - test/404.jp                       21% 877 - test/409.jp                       21% 888 - test/419.jp                       21% 893 - test/423.jp                       21% 898 - test/428.jp                       21% 904 - test/433.jp                       21% 910 - test/439.jp                       22% 920 - test/448.jp                       22% 927 - test/454.jp                       22% 936 - test/462.jp                       22% 937 - test/463.jp                       22% 945 - test/470.jp                       22% 950 - test/475.jp                       22% 954 - test/479.jp                       23% 965 - test/489.jp                       23% 971 - test/494.jp                       23% 975 - test/498.jp                       23% 979 - test/500.jp                       23% 983 - test/504.jp                       23% 988 - test/509.jp                       23% 993 - test/513.jp                       23% 1002 - test/521.j                       24% 1003 - test/522.j                       24% 1007 - test/526.j                       24% 1013 - test/531.j                       24% 1022 - test/54.jp                       24% 1023 - test/540.j                       24% 1030 - test/547.j                       24% 1031 - test/548.j                       24% 1037 - test/553.j                       24% 1042 - test/558.j                       25% 1052 - test/567.j                       25% 1057 - test/571.j                       25% 1061 - test/575.j                       25% 1066 - test/58.jp                       25% 1072 - test/585.j                       25% 1076 - test/589.j                       25% 1081 - test/593.j                       26% 1085 - test/597.j                       26% 1089 - test/60.jp                       26% 1095 - test/605.j                       26% 1099 - test/609.j                       26% 1104 - test/613.j                       26% 1106 - test/615.j                       26% 1114 - test/622.j                       26% 1120 - test/628.j                       26% 1124 - test/631.j                       27% 1128 - test/635.j                       27% 1135 - test/641.j                       27% 1138 - test/644.j                       27% 1146 - test/651.j                       27% 1151 - test/656.j                       27% 1155 - test/66.jp                       27% 1160 - test/664.j                       27% 1164 - test/668.j                       28% 1169 - test/672.j                       28% 1176 - test/679.j                       28% 1181 - test/683.j                       28% 1190 - test/691.j                       28% 1193 - test/694.j                       28% 1200 - test/70.jp                       28% 1206 - test/705.j                       29% 1210 - test/709.j                       29% 1215 - test/713.j                       29% 1219 - test/717.j                       29% 1233 - test/73.jp                       29% 1238 - test/734.j                       29% 1244 - test/74.jp                       29% 1248 - test/743.j                       30% 1255 - test/75.jp                       30% 1262 - test/756.j                       30% 1269 - test/762.j                       30% 1270 - test/763.j                       30% 1278 - test/770.j                       30% 1282 - test/774.j                       30% 1286 - test/778.j                       30% 1291 - test/782.j                       31% 1296 - test/787.j                       31% 1302 - test/792.j                       31% 1310 - test/8.j                     31% 1312 - test/800.j                       31% 1321 - test/809.j                       31% 1327 - test/814.j                       31% 1331 - test/818.j                       31% 1335 - test/821.j                       32% 1341 - test/827.j                       32% 1346 - test/831.j                       32% 1357 - test/841.j                       32% 1363 - test/847.j                       32% 1368 - test/851.j                       32% 1369 - test/852.j                       32% 1372 - test/855.j                       33% 1381 - test/863.j                       33% 1386 - test/868.j                       33% 1390 - test/871.j                       33% 1396 - test/877.j                       33% 1400 - test/880.j                       33% 1411 - test/890.j                       33% 1415 - test/894.j                       33% 1420 - test/899.j                       34% 1424 - test/901.j                       34% 1430 - test/907.j                       34% 1434 - test/910.j                       34% 1439 - test/915.j                       34% 1443 - test/919.j                       34% 1448 - test/923.j                       34% 1453 - test/928.j                       34% 1459 - test/933.j                       35% 1463 - test/937.j                       35% 1474 - test/947.j                       35% 1476 - test/949.j                       35% 1484 - test/956.j                       35% 1489 - test/960.j                       35% 1491 - test/962.j                       35% 1493 - test/964.j                       35% 1502 - test/972.j                       36% 1508 - test/978.j                       36% 1513 - test/982.j                       36% 1518 - test/987.j                       36% 1519 - test/988.j                       36% 1528 - test/996.j                      Everything is Ok\n",
      "\n",
      "Extracting archive: train.7z\n",
      "--\n",
      "Path = train.7z\n",
      "Type = 7z\n",
      "Physical Size = 2126278821\n",
      "Headers Size = 24792\n",
      "Method = LZMA2:24\n",
      "Solid = +\n",
      "Blocks = 2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3   36%     36% 4 - train/1000.jp                       36% 8 - train/1004.jp                       37% 13 - train/1009.j                       37% 17 - train/1012.j                       37% 21 - train/1016.j                       37% 25 - train/102.jp                       37% 29 - train/1023.j                       37% 33 - train/1027.j                       37% 37 - train/1030.j                       37% 41 - train/1034.j                       37% 45 - train/1038.j                       38% 49 - train/1041.j                       38% 53 - train/1045.j                       38% 58 - train/105.jp                       38% 62 - train/1053.j                       38% 67 - train/1058.j                       38% 72 - train/1062.j                       38% 75 - train/1065.j                       38% 80 - train/107.jp                       39% 84 - train/1073.j                       39% 87 - train/1076.j                       39% 93 - train/1081.j                       39% 96 - train/1084.j                       39% 100 - train/1088.jp                         39% 105 - train/1092.jp                         39% 109 - train/1096.jp                         39% 112 - train/1099.jp                         39% 117 - train/1102.jp                         40% 121 - train/1106.jp                         40% 124 - train/1109.jp                         40% 129 - train/1113.jp                         40% 133 - train/1117.jp                         40% 137 - train/1120.jp                         40% 141 - train/1124.jp                         40% 147 - train/113.j                       40% 151 - train/1133.jp                         40% 154 - train/1136.jp                         41% 159 - train/1140.jp                         41% 163 - train/1144.jp                         41% 168 - train/1149.jp                         41% 170 - train/1150.jp                         41% 174 - train/1154.jp                         41% 179 - train/1159.jp                         41% 183 - train/1162.jp                         41% 188 - train/1167.jp                         42% 192 - train/1170.jp                         42% 195 - train/1173.jp                         42% 201 - train/1179.jp                         42% 207 - train/1184.jp                         42% 209 - train/1186.jp                         42% 213 - train/119.j                       42% 217 - train/1193.jp                         42% 222 - train/1198.jp                         42% 225 - train/120.j                       43% 229 - train/1203.jp                         43% 234 - train/1208.jp                         43% 236 - train/121.j                       43% 242 - train/1215.jp                         43% 245 - train/1218.jp                         43% 249 - train/1221.jp                         43% 255 - train/1227.jp                         43% 258 - train/123.j                       43% 262 - train/1233.jp                         44% 266 - train/1237.jp                         44% 271 - train/1241.jp                         44% 275 - train/1245.jp                         44% 279 - train/1249.jp                         44% 284 - train/1253.jp                         44% 287 - train/1256.jp                         44% 291 - train/126.j                       44% 297 - train/1265.jp                         44% 301 - train/1269.jp                         45% 305 - train/1272.jp                         45% 309 - train/1276.jp                         45% 314 - train/1280.jp                         45% 318 - train/1284.jp                         45% 322 - train/1288.jp                         45% 327 - train/1292.jp                         45% 331 - train/1296.jp                         45% 335 - train/13.jp                       45% 339 - train/1302.jp                         46% 343 - train/1306.jp                         46% 347 - train/131.j                       46% 351 - train/1313.jp                         46% 356 - train/1318.jp                         46% 360 - train/1321.jp                         46% 364 - train/1325.jp                         46% 368 - train/1329.jp                         46% 372 - train/1332.jp                         47% 378 - train/1338.jp                         47% 382 - train/1341.jp                         47% 385 - train/1344.jp                         47% 390 - train/1349.jp                         47% 394 - train/1352.jp                         47% 398 - train/1356.jp                         47% 403 - train/1360.jp                         47% 407 - train/1364.jp                         47% 411 - train/1368.jp                         48% 415 - train/1371.jp                         48% 419 - train/1375.jp                         48% 423 - train/1379.jp                         48% 427 - train/1382.jp                         48% 431 - train/1386.jp                         48% 436 - train/1390.jp                         48% 440 - train/1394.jp                         48% 444 - train/1398.jp                         48% 448 - train/1400.jp                         49% 452 - train/1404.jp                         49% 456 - train/1408.jp                         49% 460 - train/1411.jp                         49% 464 - train/1415.jp                         49% 468 - train/1419.jp                         49% 472 - train/1422.jp                         49% 477 - train/1427.jp                         49% 482 - train/1431.jp                         50% 486 - train/1435.jp                         50% 489 - train/1438.jp                         50% 493 - train/1441.jp                         50% 497 - train/1445.jp                         50% 502 - train/145.j                       50% 506 - train/1453.jp                         50% 510 - train/1457.jp                         50% 514 - train/1460.jp                         50% 518 - train/1464.jp                         51% 522 - train/1468.jp                         51% 526 - train/1471.jp                         51% 532 - train/1477.jp                         51% 535 - train/148.j                       51% 538 - train/1482.jp                         51% 543 - train/1487.jp                         51% 547 - train/1490.jp                         51% 551 - train/1494.jp                         52% 555 - train/1498.jp                         52% 560 - train/1501.jp                         52% 563 - train/1504.jp                         52% 568 - train/1509.jp                         52% 572 - train/1512.jp                         52% 577 - train/1517.jp                         52% 580 - train/152.j                       52% 584 - train/1523.jp                         52% 588 - train/1527.jp                         53% 592 - train/1530.jp                         53% 596 - train/1534.jp                         53% 601 - train/1539.jp                         53% 605 - train/1542.jp                         53% 609 - train/1546.jp                         53% 614 - train/1550.jp                         53% 617 - train/1553.jp                         53% 622 - train/1558.jp                         53% 627 - train/1562.jp                         54% 631 - train/1566.jp                         54% 635 - train/157.j                       54% 640 - train/1574.jp                         54% 644 - train/1578.jp                         54% 647 - train/1580.jp                         54% 652 - train/1585.jp                         54% 657 - train/159.j                       54% 662 - train/1594.jp                         55% 666 - train/1598.jp                         55% 671 - train/1601.jp                         55% 674 - train/1604.jp                         55% 678 - train/1608.jp                         55% 684 - train/1613.jp                         55% 688 - train/1617.jp                         55% 691 - train/162.j                       55% 696 - train/1624.jp                         55% 699 - train/1627.jp                         56% 703 - train/1630.jp                         56% 707 - train/1634.jp                         56% 711 - train/1638.jp                         56% 715 - train/1641.jp                         56% 719 - train/1645.jp                         56% 723 - train/1649.jp                         56% 727 - train/1652.jpg"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b                         56% 731 - train/1656.jp                         56% 735 - train/166.j                       57% 739 - train/1663.jp                         57% 744 - train/1668.jp                         57% 748 - train/1671.jp                         57% 752 - train/1675.jp                         57% 756 - train/1679.jp                         57% 759 - train/1681.jp                         57% 764 - train/1686.jp                         57% 768 - train/169.j                       58% 772 - train/1693.jp                         58% 777 - train/1698.jp                         58% 782 - train/1701.jp                         58% 786 - train/1705.jp                         58% 790 - train/1709.jp                         58% 795 - train/1713.jp                         58% 798 - train/1716.jp                         58% 802 - train/172.j                       58% 806 - train/1723.jp                         59% 810 - train/1727.jp                         59% 814 - train/1730.jp                         59% 818 - train/1734.jp                         59% 824 - train/174.j                       59% 827 - train/1742.jp                         59% 830 - train/1745.jp                         59% 834 - train/1749.jp                         59% 839 - train/1753.jp                         59% 843 - train/1757.jp                         60% 847 - train/1760.jp                         60% 851 - train/1764.jp                         60% 855 - train/1768.jp                         60% 860 - train/1772.jp                         60% 863 - train/1775.jp                         60% 868 - train/178.j                       60% 872 - train/1783.jp                         60% 876 - train/1787.jp                         60% 880 - train/1790.jp                         61% 884 - train/1794.jp                         61% 888 - train/1798.jp                         61% 893 - train/1801.jp                         61% 897 - train/1805.jp                         61% 902 - train/181.j                       61% 906 - train/1813.jp                         61% 909 - train/1816.jp                         61% 914 - train/1820.jp                         62% 918 - train/1824.jp                         62% 923 - train/1829.jp                         62% 926 - train/1831.jp                         62% 930 - train/1835.jp                         62% 933 - train/1838.jp                         62% 938 - train/1842.jp                         62% 943 - train/1847.jp                         62% 947 - train/1850.jp                         62% 951 - train/1854.jp                         63% 956 - train/1859.jp                         63% 960 - train/1862.jp                         63% 963 - train/1865.jp                         63% 968 - train/187.j                       63% 972 - train/1873.jp                         63% 976 - train/1877.jp                         63% 981 - train/1881.jp                         63% 985 - train/1885.jp                         63% 988 - train/1888.jp                         64% 992 - train/1891.jp                         64% 996 - train/1895.jp                         64% 1000 - train/1899.j                         64% 1004 - train/1901.j                         64% 1010 - train/1907.j                         64% 1013 - train/191.jp                         64% 1017 - train/1913.j                         64% 1021 - train/1917.j                         64% 1025 - train/1920.j                         65% 1030 - train/1925.j                         65% 1034 - train/1929.j                         65% 1039 - train/1933.j                         65% 1044 - train/1938.j                         65% 1048 - train/1941.j                         65% 1052 - train/1945.j                         65% 1056 - train/1949.j                         65% 1060 - train/1952.j                         66% 1065 - train/1957.j                         66% 1069 - train/1960.j                         66% 1073 - train/1964.j                         66% 1077 - train/1968.j                         66% 1078 - train/1969.j                         66% 1083 - train/1973.j                         66% 1086 - train/1976.j                         66% 1089 - train/1979.j                         66% 1093 - train/1982.j                         66% 1098 - train/1987.j                         67% 1101 - train/199.jp                         67% 1106 - train/1994.j                         67% 1110 - train/1998.j                         67% 1114 - train/200.jp                         67% 1118 - train/2003.j                         67% 1122 - train/2007.j                         67% 1126 - train/2010.j                         67% 1131 - train/2015.j                         68% 1135 - train/2019.j                         68% 1140 - train/2023.j                         68% 1144 - train/2027.j                         68% 1148 - train/2030.j                         68% 1152 - train/2034.j                         68% 1156 - train/2038.j                         68% 1160 - train/2041.j                         68% 1164 - train/2045.j                         68% 1169 - train/205.jp                         69% 1172 - train/2052.j                         69% 1177 - train/2057.j                         69% 1181 - train/2060.j                         69% 1185 - train/2064.j                         69% 1190 - train/2069.j                         69% 1194 - train/2072.j                         69% 1197 - train/2075.j                         69% 1202 - train/208.jp                         69% 1205 - train/2082.j                         70% 1210 - train/2087.j                         70% 1214 - train/2090.j                         70% 1218 - train/2094.j                         70% 1222 - train/2098.j                         70% 1227 - train/2101.j                         70% 1231 - train/2105.j                         70% 1235 - train/2109.j                         70% 1240 - train/2113.j                         71% 1244 - train/2117.j                         71% 1248 - train/2120.j                         71% 1252 - train/2124.j                         71% 1256 - train/2128.j                         71% 1260 - train/2131.j                         71% 1264 - train/2135.j                         71% 1267 - train/2138.j                         71% 1271 - train/2141.j                         71% 1276 - train/2146.j                         72% 1280 - train/215.jp                         72% 1284 - train/2153.j                         72% 1288 - train/2157.j                         72% 1292 - train/2160.j                         72% 1296 - train/2164.j                         72% 1300 - train/2168.j                         72% 1304 - train/2171.j                         72% 1308 - train/2175.j                         72% 1312 - train/2179.j                         73% 1316 - train/2182.j                         73% 1320 - train/2186.j                         73% 1325 - train/2190.j                         73% 1328 - train/2193.j                         73% 1332 - train/2197.j                         73% 1336 - train/220.jp                         73% 1340 - train/2203.j                         73% 1344 - train/2207.j                         74% 1349 - train/2211.j                         74% 1353 - train/2215.j                         74% 1357 - train/2219.j                         74% 1361 - train/2222.j                         74% 1365 - train/2226.j                         74% 1369 - train/223.jp                         74% 1374 - train/2234.j                         74% 1378 - train/2238.j                         74% 1384 - train/2243.j                         75% 1387 - train/2246.j                         75% 1392 - train/2250.j                         75% 1395 - train/2253.j                         75% 1399 - train/2257.j                         75% 1404 - train/2261.j                         75% 1409 - train/2266.j                         75% 1412 - train/2269.j                         75% 1416 - train/2272.j                         75% 1420 - train/2276.j                         76% 1424 - train/228.jp                         76% 1428 - train/2283.j                         76% 1432 - train/2287.j                         76% 1437 - train/2291.jpg"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         76% 1442 - train/23.j                       76% 1445 - train/232.jp                         76% 1449 - train/236.jp                         76% 1453 - train/24.j                       76% 1458 - train/244.jp                         77% 1462 - train/248.jp                         77% 1466 - train/251.jp                         77% 1470 - train/255.jp                         77% 1475 - train/26.j                       77% 1479 - train/263.jp                         77% 1483 - train/267.jp                         77% 1488 - train/271.jp                         77% 1491 - train/274.jp                         78% 1495 - train/278.jp                         78% 1499 - train/281.jp                         78% 1504 - train/286.jp                         78% 1508 - train/29.j                       78% 1512 - train/293.jp                         78% 1516 - train/297.jp                         78% 1520 - train/30.j                       78% 1521 - train/300.jp                         78% 1526 - train/305.jp                         78% 1530 - train/309.jp                         79% 1533 - train/311.jp                         79% 1538 - train/316.jp                         79% 1541 - train/319.jp                         79% 1545 - train/322.jp                         79% 1549 - train/326.jp                         79% 1553 - train/33.j                       79% 1558 - train/334.jp                         79% 1561 - train/337.jp                         79% 1565 - train/340.jp                         80% 1570 - train/345.jp                         80% 1575 - train/35.j                       80% 1579 - train/353.jp                         80% 1583 - train/357.jp                         80% 1588 - train/361.jp                         80% 1590 - train/363.jp                         80% 1594 - train/367.jp                         80% 1599 - train/371.jp                         81% 1603 - train/375.jp                         81% 1607 - train/379.jp                         81% 1612 - train/383.jp                         81% 1616 - train/387.jp                         81% 1621 - train/391.jp                         81% 1625 - train/395.jp                         81% 1629 - train/399.jp                         81% 1633 - train/401.jp                         81% 1637 - train/405.jp                         82% 1641 - train/409.jp                         82% 1645 - train/412.jp                         82% 1650 - train/417.jp                         82% 1654 - train/420.jp                         82% 1658 - train/424.jp                         82% 1663 - train/429.jp                         82% 1667 - train/432.jp                         82% 1671 - train/436.jp                         82% 1675 - train/44.j                       83% 1679 - train/443.jp                         83% 1684 - train/448.jp                         83% 1688 - train/451.jp                         83% 1692 - train/455.jp                         83% 1696 - train/459.jp                         83% 1700 - train/462.jp                         83% 1704 - train/466.jp                         83% 1708 - train/47.j                       84% 1712 - train/473.jp                         84% 1717 - train/478.jp                         84% 1721 - train/481.jp                         84% 1725 - train/485.jp                         84% 1729 - train/489.jp                         84% 1733 - train/492.jp                         84% 1738 - train/497.jp                         84% 1742 - train/50.j                       84% 1746 - train/503.jp                         85% 1750 - train/507.jp                         85% 1754 - train/510.jp                         85% 1758 - train/514.jp                         85% 1762 - train/518.jp                         85% 1766 - train/521.jp                         85% 1770 - train/525.jp                         85% 1774 - train/529.jp                         85% 1778 - train/532.jp                         85% 1783 - train/537.jp                         86% 1786 - train/54.j                       86% 1790 - train/543.jp                         86% 1795 - train/548.jp                         86% 1798 - train/550.jp                         86% 1802 - train/554.jp                         86% 1806 - train/558.jp                         86% 1812 - train/563.jp                         86% 1815 - train/566.jp                         86% 1819 - train/57.j                       87% 1824 - train/574.jp                         87% 1828 - train/578.jp                         87% 1833 - train/582.jp                         87% 1835 - train/584.jp                         87% 1837 - train/586.jp                         87% 1841 - train/59.j                       87% 1845 - train/593.jp                         87% 1850 - train/598.jp                         87% 1854 - train/600.jp                         88% 1858 - train/604.jp                         88% 1862 - train/608.jp                         88% 1866 - train/611.jp                         88% 1871 - train/616.jp                         88% 1875 - train/62.j                       88% 1878 - train/622.jp                         88% 1883 - train/627.jp                         88% 1887 - train/630.jp                         88% 1891 - train/634.jp                         89% 1896 - train/639.jp                         89% 1900 - train/642.jp                         89% 1904 - train/646.jp                         89% 1907 - train/649.jp                         89% 1912 - train/653.jp                         89% 1916 - train/657.jp                         89% 1921 - train/661.jp                         89% 1925 - train/665.jp                         89% 1928 - train/668.jp                         90% 1932 - train/671.jp                         90% 1936 - train/675.jp                         90% 1941 - train/68.j                       90% 1942 - train/680.jp                         90% 1945 - train/683.jp                         90% 1949 - train/687.jp                         90% 1954 - train/691.jp                         90% 1958 - train/695.jp                         90% 1962 - train/699.jp                         91% 1966 - train/701.jp                         91% 1971 - train/706.jp                         91% 1976 - train/710.jp                         91% 1980 - train/714.jp                         91% 1983 - train/717.jp                         91% 1988 - train/721.jp                         91% 1992 - train/725.jp                         91% 1996 - train/729.jp                         91% 2000 - train/732.jp                         92% 2004 - train/736.jp                         92% 2008 - train/74.j                       92% 2009 - train/740.jp                         92% 2013 - train/744.jp                         92% 2018 - train/749.jp                         92% 2022 - train/752.jp                         92% 2025 - train/755.jp                         92% 2029 - train/759.jp                         92% 2032 - train/761.jp                         92% 2037 - train/766.jp                         93% 2041 - train/77.j                       93% 2045 - train/773.jp                         93% 2049 - train/777.jp                         93% 2054 - train/781.jp                         93% 2057 - train/784.jp                         93% 2062 - train/789.jp                         93% 2066 - train/792.jp                         93% 2070 - train/796.jp                         94% 2074 - train/8.jp                       94% 2078 - train/802.jp                         94% 2082 - train/806.jp                         94% 2086 - train/81.j                       94% 2091 - train/814.jp                         94% 2095 - train/818.jp                         94% 2100 - train/822.jp                         94% 2104 - train/826.jp                         94% 2108 - train/83.j                       95% 2112 - train/833.jp                         95% 2116 - train/837.jp                         95% 2119 - train/84.j                       95% 2121 - train/841.jp                         95% 2125 - train/845.jp                         95% 2130 - train/85.j                       95% 2133 - train/852.jp                         95% 2137 - train/856.jpg"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b                         95% 2141 - train/86.j                       95% 2145 - train/863.jp                         96% 2149 - train/867.jp                         96% 2153 - train/870.jp                         96% 2158 - train/875.jp                         96% 2162 - train/879.jp                         96% 2167 - train/883.jp                         96% 2171 - train/887.jp                         96% 2175 - train/890.jp                         96% 2179 - train/894.jp                         96% 2180 - train/895.jp                         97% 2183 - train/898.jp                         97% 2188 - train/901.jp                         97% 2191 - train/904.jp                         97% 2195 - train/908.jp                         97% 2199 - train/911.jp                         97% 2204 - train/916.jp                         97% 2208 - train/92.j                       97% 2211 - train/922.jp                         97% 2213 - train/924.jp                         97% 2218 - train/929.jp                         98% 2221 - train/931.jp                         98% 2225 - train/935.jp                         98% 2230 - train/94.j                       98% 2234 - train/943.jp                         98% 2238 - train/947.jp                         98% 2242 - train/950.jp                         98% 2246 - train/954.jp                         98% 2251 - train/959.jp                         98% 2255 - train/962.jp                         99% 2259 - train/966.jp                         99% 2263 - train/97.j                       99% 2267 - train/973.jp                         99% 2271 - train/977.jp                         99% 2275 - train/980.jp                         99% 2279 - train/984.jp                         99% 2284 - train/989.jp                         99% 2288 - train/992.jp                         99% 2292 - train/996.jp                        Everything is Ok\n",
      "100% 229        \n",
      "Archives: 2\n",
      "OK archives: 2\n",
      "Folders: 2\n",
      "Files: 3826\n",
      "Size:       3617540255\n",
      "Compressed: 3353865207\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_DIR\n",
    "!module load centos/7\n",
    "!7za x '*.7z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n",
    "test_set = pd.read_csv(os.path.join(path, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1         0\n",
       "1     2         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img(img_path):\n",
    "    img = misc.imread(img_path)\n",
    "    img = misc.imresize(img, (128, 128))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2295/2295 [01:13<00:00, 31.03it/s]\n",
      "100%|██████████| 1531/1531 [00:47<00:00, 32.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_img, test_img = [],[]\n",
    "for img_path in tqdm(train_set['name'].iloc[:]):\n",
    "    train_img.append(read_img(os.path.join(path, 'train', str(img_path)+'.jpg')))\n",
    "\n",
    "for img_path in tqdm(test_set['name'].iloc[:]):\n",
    "    test_img.append(read_img(os.path.join(path, 'test', str(img_path)+'.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_img = np.array(train_img, np.float32)/255\n",
    "test_img = np.array(test_img, np.float32)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = np.array(train_set['invasive'].iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(filter_depth, filter_size, pool_size, activation, inputs):\n",
    "    x = Convolution2D(filter_depth, filter_size, activation=activation)(inputs)\n",
    "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_block(units, activation, drop_prob, inputs):\n",
    "    x = Dense(units, activation=activation)(inputs)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_block(units, activation, inputs):\n",
    "    x = Dense(units, activation=activation)(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_optimizer(opt, lr):\n",
    "    if opt == 'sgd':\n",
    "        return optimizers.SGD(lr=lr, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    elif opt == 'adam':\n",
    "        return optimizers.Adam(lr=lr)\n",
    "    elif opt == 'adagrad':\n",
    "        return optimizers.Adagrad(lr=lr)\n",
    "    elif opt == 'rmsprop':\n",
    "        return optimizers.RMSprop(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(input_shape, optimizer):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    m = conv_block(16, (3,3), (2,2),'relu', inputs=inputs)\n",
    "    m = conv_block(32, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = conv_block(64, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = conv_block(128, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = Flatten()(m)\n",
    "    m = dense_block(2048, 'relu', 0.65, inputs=m)\n",
    "    m = dense_block(512, 'relu', 0.55, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try a few rounds of training\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(train_img, train_label, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5s - loss: 0.6598 - acc: 0.6137 - val_loss: 0.6654 - val_acc: 0.6035\n",
      "Epoch 2/25\n",
      "4s - loss: 0.6360 - acc: 0.6380 - val_loss: 0.6375 - val_acc: 0.6035\n",
      "Epoch 3/25\n",
      "4s - loss: 0.6099 - acc: 0.6389 - val_loss: 0.5997 - val_acc: 0.6035\n",
      "Epoch 4/25\n",
      "4s - loss: 0.5834 - acc: 0.6577 - val_loss: 0.5709 - val_acc: 0.6623\n",
      "Epoch 5/25\n",
      "4s - loss: 0.5509 - acc: 0.7110 - val_loss: 0.5315 - val_acc: 0.7342\n",
      "Epoch 6/25\n",
      "4s - loss: 0.5231 - acc: 0.7369 - val_loss: 0.5161 - val_acc: 0.7386\n",
      "Epoch 7/25\n",
      "4s - loss: 0.5239 - acc: 0.7304 - val_loss: 0.4946 - val_acc: 0.7560\n",
      "Epoch 8/25\n",
      "4s - loss: 0.4880 - acc: 0.7704 - val_loss: 0.4556 - val_acc: 0.8126\n",
      "Epoch 9/25\n",
      "4s - loss: 0.4396 - acc: 0.7898 - val_loss: 0.6404 - val_acc: 0.6797\n",
      "Epoch 10/25\n",
      "4s - loss: 0.4397 - acc: 0.7941 - val_loss: 0.4874 - val_acc: 0.7495\n",
      "Epoch 11/25\n",
      "4s - loss: 0.4212 - acc: 0.8051 - val_loss: 0.3876 - val_acc: 0.8039\n",
      "Epoch 12/25\n",
      "4s - loss: 0.3538 - acc: 0.8417 - val_loss: 0.3149 - val_acc: 0.8758\n",
      "Epoch 13/25\n",
      "4s - loss: 0.3327 - acc: 0.8581 - val_loss: 0.3032 - val_acc: 0.8758\n",
      "Epoch 14/25\n",
      "4s - loss: 0.3521 - acc: 0.8463 - val_loss: 0.4255 - val_acc: 0.8105\n",
      "Epoch 15/25\n",
      "4s - loss: 0.3363 - acc: 0.8481 - val_loss: 0.2833 - val_acc: 0.8845\n",
      "Epoch 16/25\n",
      "4s - loss: 0.3090 - acc: 0.8713 - val_loss: 0.3001 - val_acc: 0.8824\n",
      "Epoch 17/25\n",
      "4s - loss: 0.2834 - acc: 0.8811 - val_loss: 0.2963 - val_acc: 0.8715\n",
      "Epoch 18/25\n",
      "4s - loss: 0.3050 - acc: 0.8816 - val_loss: 0.2921 - val_acc: 0.8758\n",
      "Epoch 19/25\n",
      "4s - loss: 0.2967 - acc: 0.8758 - val_loss: 0.2622 - val_acc: 0.8976\n",
      "Epoch 20/25\n",
      "4s - loss: 0.2836 - acc: 0.8841 - val_loss: 0.2476 - val_acc: 0.9085\n",
      "Epoch 21/25\n",
      "4s - loss: 0.2816 - acc: 0.8861 - val_loss: 0.2612 - val_acc: 0.8911\n",
      "Epoch 22/25\n",
      "4s - loss: 0.2778 - acc: 0.8902 - val_loss: 0.2346 - val_acc: 0.9041\n",
      "Epoch 23/25\n",
      "4s - loss: 0.2735 - acc: 0.8795 - val_loss: 0.2336 - val_acc: 0.9063\n",
      "Epoch 24/25\n",
      "4s - loss: 0.2647 - acc: 0.8959 - val_loss: 0.2286 - val_acc: 0.9085\n",
      "Epoch 25/25\n",
      "4s - loss: 0.2706 - acc: 0.8878 - val_loss: 0.2312 - val_acc: 0.8998\n"
     ]
    }
   ],
   "source": [
    "model = make_model((128,128,3), grab_optimizer('sgd', 0.01))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "\n",
    "gen = ImageDataGenerator(\n",
    "    rotation_range = 30,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    "    fill_mode = 'nearest')\n",
    "\n",
    "# only required of featurewise center or zca whitening or a few other things\n",
    "# gen.fit(x_train)\n",
    "\n",
    "hist = model.fit_generator(gen.flow(x_train, y_train, batch_size=64),\n",
    "                    steps_per_epoch=(len(x_train)//64) + 1,\n",
    "                    validation_data=(x_valid,y_valid),\n",
    "                    validation_steps=(len(x_valid)//64)+1,\n",
    "                    epochs=25,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold: 0\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6548 - val_acc: 0.6174\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6536 - acc: 0.6277 - val_loss: 0.6496 - val_acc: 0.6174\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6455 - acc: 0.6353 - val_loss: 0.6436 - val_acc: 0.6174\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6449 - acc: 0.6285 - val_loss: 0.6355 - val_acc: 0.6174\n",
      "Epoch 5/50\n",
      "5s - loss: 0.6320 - acc: 0.6301 - val_loss: 0.6229 - val_acc: 0.6174\n",
      "Epoch 6/50\n",
      "5s - loss: 0.6145 - acc: 0.6298 - val_loss: 0.5995 - val_acc: 0.6174\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5930 - acc: 0.6344 - val_loss: 0.5661 - val_acc: 0.6261\n",
      "Epoch 8/50\n",
      "4s - loss: 0.5619 - acc: 0.6653 - val_loss: 0.6061 - val_acc: 0.6304\n",
      "Epoch 9/50\n",
      "4s - loss: 0.5438 - acc: 0.6959 - val_loss: 0.7257 - val_acc: 0.4217\n",
      "Epoch 10/50\n",
      "4s - loss: 0.6488 - acc: 0.6125 - val_loss: 0.6198 - val_acc: 0.6174\n",
      "Epoch 11/50\n",
      "5s - loss: 0.5918 - acc: 0.6370 - val_loss: 0.5631 - val_acc: 0.7957\n",
      "Epoch 12/50\n",
      "5s - loss: 0.5291 - acc: 0.7065 - val_loss: 0.4791 - val_acc: 0.7261\n",
      "Epoch 13/50\n",
      "4s - loss: 0.4921 - acc: 0.7483 - val_loss: 0.4956 - val_acc: 0.7957\n",
      "Epoch 14/50\n",
      "4s - loss: 0.4686 - acc: 0.7707 - val_loss: 0.6119 - val_acc: 0.6087\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4552 - acc: 0.7669 - val_loss: 0.3939 - val_acc: 0.8391\n",
      "Epoch 16/50\n",
      "5s - loss: 0.4190 - acc: 0.8006 - val_loss: 0.3672 - val_acc: 0.8565\n",
      "Epoch 17/50\n",
      "5s - loss: 0.4271 - acc: 0.7983 - val_loss: 0.3523 - val_acc: 0.8739\n",
      "Epoch 18/50\n",
      "5s - loss: 0.3944 - acc: 0.8263 - val_loss: 0.3278 - val_acc: 0.8826\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3727 - acc: 0.8365 - val_loss: 0.4418 - val_acc: 0.8391\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3732 - acc: 0.8337 - val_loss: 0.3192 - val_acc: 0.8739\n",
      "Epoch 21/50\n",
      "4s - loss: 0.3488 - acc: 0.8450 - val_loss: 0.4626 - val_acc: 0.8391\n",
      "Epoch 22/50\n",
      "5s - loss: 0.3429 - acc: 0.8548 - val_loss: 0.3112 - val_acc: 0.8826\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3271 - acc: 0.8628 - val_loss: 0.2764 - val_acc: 0.9087\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3219 - acc: 0.8696 - val_loss: 0.2625 - val_acc: 0.9130\n",
      "Epoch 25/50\n",
      "4s - loss: 0.3147 - acc: 0.8646 - val_loss: 0.2519 - val_acc: 0.9130\n",
      "Epoch 26/50\n",
      "4s - loss: 0.3108 - acc: 0.8741 - val_loss: 0.2591 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "4s - loss: 0.3043 - acc: 0.8703 - val_loss: 0.2835 - val_acc: 0.8826\n",
      "Epoch 28/50\n",
      "5s - loss: 0.2942 - acc: 0.8837 - val_loss: 0.2397 - val_acc: 0.9174\n",
      "Epoch 29/50\n",
      "5s - loss: 0.3014 - acc: 0.8729 - val_loss: 0.2807 - val_acc: 0.8696\n",
      "Epoch 30/50\n",
      "4s - loss: 0.3043 - acc: 0.8694 - val_loss: 0.2896 - val_acc: 0.8739\n",
      "Epoch 31/50\n",
      "4s - loss: 0.3026 - acc: 0.8698 - val_loss: 0.2833 - val_acc: 0.8783\n",
      "Epoch 32/50\n",
      "4s - loss: 0.2956 - acc: 0.8753 - val_loss: 0.3509 - val_acc: 0.8652\n",
      "Epoch 33/50\n",
      "5s - loss: 0.2884 - acc: 0.8817 - val_loss: 0.2323 - val_acc: 0.9087\n",
      "Epoch 34/50\n",
      "4s - loss: 0.2813 - acc: 0.8814 - val_loss: 0.3210 - val_acc: 0.8522\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2769 - acc: 0.8830 - val_loss: 0.2410 - val_acc: 0.9043\n",
      "Epoch 36/50\n",
      "4s - loss: 0.2859 - acc: 0.8829 - val_loss: 0.2797 - val_acc: 0.9130\n",
      "Epoch 37/50\n",
      "4s - loss: 0.2903 - acc: 0.8784 - val_loss: 0.2793 - val_acc: 0.8739\n",
      "Epoch 38/50\n",
      "5s - loss: 0.2758 - acc: 0.8846 - val_loss: 0.2309 - val_acc: 0.9174\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2668 - acc: 0.8945 - val_loss: 0.2181 - val_acc: 0.9261\n",
      "Epoch 40/50\n",
      "5s - loss: 0.2631 - acc: 0.8906 - val_loss: 0.2162 - val_acc: 0.9261\n",
      "Epoch 41/50\n",
      "4s - loss: 0.2700 - acc: 0.8919 - val_loss: 0.2377 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      "5s - loss: 0.2667 - acc: 0.8907 - val_loss: 0.2578 - val_acc: 0.9174\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2727 - acc: 0.8929 - val_loss: 0.2370 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      "4s - loss: 0.2652 - acc: 0.8903 - val_loss: 0.2220 - val_acc: 0.9174\n",
      "Epoch 45/50\n",
      "5s - loss: 0.2619 - acc: 0.8911 - val_loss: 0.2456 - val_acc: 0.9087\n",
      "Epoch 46/50\n",
      "5s - loss: 0.2607 - acc: 0.8938 - val_loss: 0.2456 - val_acc: 0.9174\n",
      "Epoch 47/50\n",
      "4s - loss: 0.2533 - acc: 0.8978 - val_loss: 0.2236 - val_acc: 0.9130\n",
      "Epoch 48/50\n",
      "5s - loss: 0.2559 - acc: 0.8919 - val_loss: 0.2256 - val_acc: 0.9174\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.9708099402778088, va score: 0.9691101152368758\n",
      "\n",
      "\n",
      "kfold: 1\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6586 - acc: 0.6061 - val_loss: 0.6480 - val_acc: 0.6217\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6409 - acc: 0.6299 - val_loss: 0.6402 - val_acc: 0.6217\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6273 - acc: 0.6309 - val_loss: 0.6231 - val_acc: 0.6217\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6116 - acc: 0.6320 - val_loss: 0.6031 - val_acc: 0.6217\n",
      "Epoch 5/50\n",
      "5s - loss: 0.5896 - acc: 0.6294 - val_loss: 0.5871 - val_acc: 0.6696\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5582 - acc: 0.6684 - val_loss: 0.5397 - val_acc: 0.6565\n",
      "Epoch 7/50\n",
      "4s - loss: 0.5326 - acc: 0.7183 - val_loss: 0.6358 - val_acc: 0.6304\n",
      "Epoch 8/50\n",
      "5s - loss: 0.5491 - acc: 0.7002 - val_loss: 0.5160 - val_acc: 0.7043\n",
      "Epoch 9/50\n",
      "5s - loss: 0.4921 - acc: 0.7572 - val_loss: 0.5097 - val_acc: 0.7000\n",
      "Epoch 10/50\n",
      "5s - loss: 0.4688 - acc: 0.7851 - val_loss: 0.5025 - val_acc: 0.7130\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4597 - acc: 0.7796 - val_loss: 0.4555 - val_acc: 0.7783\n",
      "Epoch 12/50\n",
      "5s - loss: 0.4407 - acc: 0.7959 - val_loss: 0.4352 - val_acc: 0.7957\n",
      "Epoch 13/50\n",
      "4s - loss: 0.4212 - acc: 0.8057 - val_loss: 0.4191 - val_acc: 0.7783\n",
      "Epoch 14/50\n",
      "5s - loss: 0.4398 - acc: 0.7951 - val_loss: 0.4158 - val_acc: 0.7783\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4001 - acc: 0.8247 - val_loss: 0.4016 - val_acc: 0.7957\n",
      "Epoch 16/50\n",
      "4s - loss: 0.3979 - acc: 0.8149 - val_loss: 0.4217 - val_acc: 0.8000\n",
      "Epoch 17/50\n",
      "5s - loss: 0.3984 - acc: 0.8135 - val_loss: 0.3821 - val_acc: 0.8304\n",
      "Epoch 18/50\n",
      "4s - loss: 0.3677 - acc: 0.8381 - val_loss: 0.3848 - val_acc: 0.8348\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3495 - acc: 0.8468 - val_loss: 0.3756 - val_acc: 0.8304\n",
      "Epoch 20/50\n",
      "4s - loss: 0.3460 - acc: 0.8533 - val_loss: 0.3862 - val_acc: 0.8304\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3442 - acc: 0.8567 - val_loss: 0.3455 - val_acc: 0.8565\n",
      "Epoch 22/50\n",
      "4s - loss: 0.3214 - acc: 0.8586 - val_loss: 0.3945 - val_acc: 0.8391\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3097 - acc: 0.8695 - val_loss: 0.3264 - val_acc: 0.8478\n",
      "Epoch 24/50\n",
      "4s - loss: 0.3167 - acc: 0.8654 - val_loss: 0.3562 - val_acc: 0.8565\n",
      "Epoch 25/50\n",
      "5s - loss: 0.3010 - acc: 0.8721 - val_loss: 0.3220 - val_acc: 0.8565\n",
      "Epoch 26/50\n",
      "4s - loss: 0.2901 - acc: 0.8779 - val_loss: 0.3290 - val_acc: 0.8609\n",
      "Epoch 27/50\n",
      "5s - loss: 0.2874 - acc: 0.8776 - val_loss: 0.3005 - val_acc: 0.8391\n",
      "Epoch 28/50\n",
      "4s - loss: 0.2905 - acc: 0.8810 - val_loss: 0.3084 - val_acc: 0.8478\n",
      "Epoch 29/50\n",
      "5s - loss: 0.2760 - acc: 0.8870 - val_loss: 0.2945 - val_acc: 0.8522\n",
      "Epoch 30/50\n",
      "4s - loss: 0.2925 - acc: 0.8785 - val_loss: 0.3137 - val_acc: 0.8696\n",
      "Epoch 31/50\n",
      "4s - loss: 0.2821 - acc: 0.8846 - val_loss: 0.3134 - val_acc: 0.8435\n",
      "Epoch 32/50\n",
      "5s - loss: 0.2811 - acc: 0.8895 - val_loss: 0.2861 - val_acc: 0.8478\n",
      "Epoch 33/50\n",
      "4s - loss: 0.2786 - acc: 0.8848 - val_loss: 0.4599 - val_acc: 0.7696\n",
      "Epoch 34/50\n",
      "4s - loss: 0.2800 - acc: 0.8846 - val_loss: 0.2902 - val_acc: 0.8826\n",
      "Epoch 35/50\n",
      "4s - loss: 0.2710 - acc: 0.8882 - val_loss: 0.3004 - val_acc: 0.8609\n",
      "Epoch 36/50\n",
      "4s - loss: 0.2613 - acc: 0.8953 - val_loss: 0.3371 - val_acc: 0.8348\n",
      "Epoch 37/50\n",
      "5s - loss: 0.2637 - acc: 0.8975 - val_loss: 0.4087 - val_acc: 0.8522\n",
      "Epoch 38/50\n",
      "4s - loss: 0.2646 - acc: 0.8957 - val_loss: 0.3165 - val_acc: 0.8391\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2566 - acc: 0.8907 - val_loss: 0.2860 - val_acc: 0.8652\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2528 - acc: 0.9006 - val_loss: 0.4600 - val_acc: 0.8261\n",
      "Epoch 41/50\n",
      "4s - loss: 0.2513 - acc: 0.9050 - val_loss: 0.2968 - val_acc: 0.8870\n",
      "Epoch 42/50\n",
      "4s - loss: 0.2441 - acc: 0.9042 - val_loss: 0.2910 - val_acc: 0.8913\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2355 - acc: 0.9074 - val_loss: 0.3164 - val_acc: 0.8957\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2518 - acc: 0.9012 - val_loss: 0.2653 - val_acc: 0.8739\n",
      "Epoch 45/50\n",
      "4s - loss: 0.2454 - acc: 0.9048 - val_loss: 0.2792 - val_acc: 0.8652\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2525 - acc: 0.8934 - val_loss: 0.2863 - val_acc: 0.8739\n",
      "Epoch 47/50\n",
      "4s - loss: 0.2411 - acc: 0.9066 - val_loss: 0.3138 - val_acc: 0.8913\n",
      "Epoch 48/50\n",
      "5s - loss: 0.2403 - acc: 0.9040 - val_loss: 0.2553 - val_acc: 0.8739\n",
      "Epoch 49/50\n",
      "4s - loss: 0.2371 - acc: 0.9025 - val_loss: 0.2629 - val_acc: 0.8696\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2488 - acc: 0.9055 - val_loss: 0.2631 - val_acc: 0.8739\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.976818374026865, va score: 0.9676071055381401\n",
      "\n",
      "\n",
      "kfold: 2\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6622 - acc: 0.6157 - val_loss: 0.6456 - val_acc: 0.6261\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s - loss: 0.6471 - acc: 0.6309 - val_loss: 0.6387 - val_acc: 0.6261\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6403 - acc: 0.6340 - val_loss: 0.6304 - val_acc: 0.6261\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6307 - acc: 0.6309 - val_loss: 0.6149 - val_acc: 0.6261\n",
      "Epoch 5/50\n",
      "5s - loss: 0.6151 - acc: 0.6311 - val_loss: 0.5871 - val_acc: 0.6261\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5935 - acc: 0.6397 - val_loss: 0.5508 - val_acc: 0.6174\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5621 - acc: 0.6711 - val_loss: 0.5198 - val_acc: 0.6652\n",
      "Epoch 8/50\n",
      "4s - loss: 0.5435 - acc: 0.7099 - val_loss: 0.5920 - val_acc: 0.6391\n",
      "Epoch 9/50\n",
      "4s - loss: 0.5185 - acc: 0.7396 - val_loss: 0.5252 - val_acc: 0.6696\n",
      "Epoch 10/50\n",
      "5s - loss: 0.4980 - acc: 0.7560 - val_loss: 0.4484 - val_acc: 0.7609\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4852 - acc: 0.7562 - val_loss: 0.3979 - val_acc: 0.8565\n",
      "Epoch 12/50\n",
      "5s - loss: 0.4711 - acc: 0.7786 - val_loss: 0.3761 - val_acc: 0.8304\n",
      "Epoch 13/50\n",
      "5s - loss: 0.4475 - acc: 0.7936 - val_loss: 0.3604 - val_acc: 0.8348\n",
      "Epoch 14/50\n",
      "4s - loss: 0.4152 - acc: 0.8123 - val_loss: 0.3859 - val_acc: 0.8304\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4213 - acc: 0.8011 - val_loss: 0.3437 - val_acc: 0.8696\n",
      "Epoch 16/50\n",
      "4s - loss: 0.3969 - acc: 0.8279 - val_loss: 0.3502 - val_acc: 0.8522\n",
      "Epoch 17/50\n",
      "5s - loss: 0.3725 - acc: 0.8378 - val_loss: 0.3529 - val_acc: 0.8522\n",
      "Epoch 18/50\n",
      "5s - loss: 0.3550 - acc: 0.8494 - val_loss: 0.2922 - val_acc: 0.8783\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3443 - acc: 0.8452 - val_loss: 0.2636 - val_acc: 0.9043\n",
      "Epoch 20/50\n",
      "4s - loss: 0.3298 - acc: 0.8636 - val_loss: 0.3045 - val_acc: 0.8957\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3316 - acc: 0.8591 - val_loss: 0.2602 - val_acc: 0.8913\n",
      "Epoch 22/50\n",
      "4s - loss: 0.3187 - acc: 0.8708 - val_loss: 0.2650 - val_acc: 0.9087\n",
      "Epoch 23/50\n",
      "5s - loss: 0.2997 - acc: 0.8776 - val_loss: 0.2575 - val_acc: 0.9087\n",
      "Epoch 24/50\n",
      "4s - loss: 0.2999 - acc: 0.8669 - val_loss: 0.2581 - val_acc: 0.9174\n",
      "Epoch 25/50\n",
      "5s - loss: 0.2959 - acc: 0.8882 - val_loss: 0.2551 - val_acc: 0.9043\n",
      "Epoch 26/50\n",
      "4s - loss: 0.2944 - acc: 0.8787 - val_loss: 0.2721 - val_acc: 0.9043\n",
      "Epoch 27/50\n",
      "5s - loss: 0.2890 - acc: 0.8881 - val_loss: 0.2886 - val_acc: 0.9130\n",
      "Epoch 28/50\n",
      "5s - loss: 0.2869 - acc: 0.8786 - val_loss: 0.2432 - val_acc: 0.9174\n",
      "Epoch 29/50\n",
      "5s - loss: 0.2877 - acc: 0.8915 - val_loss: 0.2358 - val_acc: 0.9087\n",
      "Epoch 30/50\n",
      "4s - loss: 0.2812 - acc: 0.8844 - val_loss: 0.2697 - val_acc: 0.9043\n",
      "Epoch 31/50\n",
      "5s - loss: 0.2745 - acc: 0.8927 - val_loss: 0.2347 - val_acc: 0.9174\n",
      "Epoch 32/50\n",
      "4s - loss: 0.2719 - acc: 0.8924 - val_loss: 0.2468 - val_acc: 0.9217\n",
      "Epoch 33/50\n",
      "5s - loss: 0.2791 - acc: 0.8824 - val_loss: 0.3439 - val_acc: 0.8957\n",
      "Epoch 34/50\n",
      "5s - loss: 0.2808 - acc: 0.8878 - val_loss: 0.2425 - val_acc: 0.9217\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2612 - acc: 0.8988 - val_loss: 0.2321 - val_acc: 0.9217\n",
      "Epoch 36/50\n",
      "4s - loss: 0.2798 - acc: 0.8889 - val_loss: 0.2437 - val_acc: 0.9174\n",
      "Epoch 37/50\n",
      "4s - loss: 0.2693 - acc: 0.8864 - val_loss: 0.3337 - val_acc: 0.9087\n",
      "Epoch 38/50\n",
      "5s - loss: 0.2615 - acc: 0.8934 - val_loss: 0.2360 - val_acc: 0.9130\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2568 - acc: 0.8960 - val_loss: 0.2204 - val_acc: 0.9304\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2635 - acc: 0.8944 - val_loss: 0.2358 - val_acc: 0.9174\n",
      "Epoch 41/50\n",
      "5s - loss: 0.2691 - acc: 0.8935 - val_loss: 0.2342 - val_acc: 0.9130\n",
      "Epoch 42/50\n",
      "4s - loss: 0.2587 - acc: 0.8961 - val_loss: 0.2462 - val_acc: 0.9174\n",
      "Epoch 43/50\n",
      "5s - loss: 0.2671 - acc: 0.8904 - val_loss: 0.2085 - val_acc: 0.9304\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2684 - acc: 0.8914 - val_loss: 0.2036 - val_acc: 0.9348\n",
      "Epoch 45/50\n",
      "4s - loss: 0.2486 - acc: 0.9051 - val_loss: 0.2208 - val_acc: 0.9174\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2592 - acc: 0.8981 - val_loss: 0.2322 - val_acc: 0.9217\n",
      "Epoch 47/50\n",
      "4s - loss: 0.2476 - acc: 0.8998 - val_loss: 0.2224 - val_acc: 0.9217\n",
      "Epoch 48/50\n",
      "4s - loss: 0.2478 - acc: 0.9028 - val_loss: 0.2297 - val_acc: 0.9261\n",
      "Epoch 49/50\n",
      "4s - loss: 0.2500 - acc: 0.8961 - val_loss: 0.2263 - val_acc: 0.9217\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2637 - acc: 0.8929 - val_loss: 0.2279 - val_acc: 0.9348\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.9701751190556076, va score: 0.9705264857881136\n",
      "\n",
      "\n",
      "kfold: 3\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6643 - acc: 0.5983 - val_loss: 0.6207 - val_acc: 0.6652\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6445 - acc: 0.6293 - val_loss: 0.6133 - val_acc: 0.6652\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6323 - acc: 0.6301 - val_loss: 0.6026 - val_acc: 0.6652\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6187 - acc: 0.6348 - val_loss: 0.5915 - val_acc: 0.6652\n",
      "Epoch 5/50\n",
      "5s - loss: 0.6091 - acc: 0.6317 - val_loss: 0.5607 - val_acc: 0.6696\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5826 - acc: 0.6406 - val_loss: 0.5302 - val_acc: 0.6870\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5562 - acc: 0.6851 - val_loss: 0.5021 - val_acc: 0.7435\n",
      "Epoch 8/50\n",
      "4s - loss: 0.5363 - acc: 0.7240 - val_loss: 0.5075 - val_acc: 0.7870\n",
      "Epoch 9/50\n",
      "5s - loss: 0.5138 - acc: 0.7462 - val_loss: 0.4815 - val_acc: 0.7304\n",
      "Epoch 10/50\n",
      "5s - loss: 0.5066 - acc: 0.7462 - val_loss: 0.4462 - val_acc: 0.7696\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4877 - acc: 0.7604 - val_loss: 0.4627 - val_acc: 0.7913\n",
      "Epoch 12/50\n",
      "5s - loss: 0.4825 - acc: 0.7708 - val_loss: 0.4030 - val_acc: 0.8304\n",
      "Epoch 13/50\n",
      "4s - loss: 0.4473 - acc: 0.7858 - val_loss: 0.4103 - val_acc: 0.8043\n",
      "Epoch 14/50\n",
      "4s - loss: 0.4383 - acc: 0.8061 - val_loss: 0.5045 - val_acc: 0.7348\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4279 - acc: 0.7947 - val_loss: 0.4138 - val_acc: 0.8130\n",
      "Epoch 16/50\n",
      "5s - loss: 0.4096 - acc: 0.8184 - val_loss: 0.3562 - val_acc: 0.8435\n",
      "Epoch 17/50\n",
      "4s - loss: 0.4157 - acc: 0.8061 - val_loss: 0.4462 - val_acc: 0.7913\n",
      "Epoch 18/50\n",
      "5s - loss: 0.4001 - acc: 0.8258 - val_loss: 0.3328 - val_acc: 0.8696\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3783 - acc: 0.8322 - val_loss: 0.3232 - val_acc: 0.8609\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3682 - acc: 0.8322 - val_loss: 0.2956 - val_acc: 0.8696\n",
      "Epoch 21/50\n",
      "4s - loss: 0.3561 - acc: 0.8497 - val_loss: 0.4762 - val_acc: 0.7261\n",
      "Epoch 22/50\n",
      "5s - loss: 0.3438 - acc: 0.8375 - val_loss: 0.2849 - val_acc: 0.8783\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3268 - acc: 0.8618 - val_loss: 0.2532 - val_acc: 0.9087\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3329 - acc: 0.8496 - val_loss: 0.2506 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "5s - loss: 0.3070 - acc: 0.8740 - val_loss: 0.2415 - val_acc: 0.9087\n",
      "Epoch 26/50\n",
      "5s - loss: 0.3076 - acc: 0.8668 - val_loss: 0.2852 - val_acc: 0.8739\n",
      "Epoch 27/50\n",
      "5s - loss: 0.3053 - acc: 0.8730 - val_loss: 0.2391 - val_acc: 0.9130\n",
      "Epoch 28/50\n",
      "5s - loss: 0.2810 - acc: 0.8843 - val_loss: 0.2260 - val_acc: 0.9087\n",
      "Epoch 29/50\n",
      "4s - loss: 0.2887 - acc: 0.8784 - val_loss: 0.2383 - val_acc: 0.8913\n",
      "Epoch 30/50\n",
      "4s - loss: 0.2931 - acc: 0.8808 - val_loss: 0.2389 - val_acc: 0.9087\n",
      "Epoch 31/50\n",
      "4s - loss: 0.2957 - acc: 0.8730 - val_loss: 0.2402 - val_acc: 0.9043\n",
      "Epoch 32/50\n",
      "5s - loss: 0.2841 - acc: 0.8826 - val_loss: 0.2282 - val_acc: 0.9261\n",
      "Epoch 33/50\n",
      "5s - loss: 0.2869 - acc: 0.8812 - val_loss: 0.2379 - val_acc: 0.9087\n",
      "Epoch 34/50\n",
      "4s - loss: 0.2787 - acc: 0.8835 - val_loss: 0.3553 - val_acc: 0.8696\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2821 - acc: 0.8860 - val_loss: 0.2233 - val_acc: 0.9261\n",
      "Epoch 36/50\n",
      "5s - loss: 0.2710 - acc: 0.8873 - val_loss: 0.2171 - val_acc: 0.9217\n",
      "Epoch 37/50\n",
      "5s - loss: 0.2791 - acc: 0.8894 - val_loss: 0.2278 - val_acc: 0.9043\n",
      "Epoch 38/50\n",
      "4s - loss: 0.2763 - acc: 0.8875 - val_loss: 0.2487 - val_acc: 0.8870\n",
      "Epoch 39/50\n",
      "4s - loss: 0.2670 - acc: 0.8889 - val_loss: 0.2233 - val_acc: 0.9087\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2658 - acc: 0.8886 - val_loss: 0.2410 - val_acc: 0.9130\n",
      "Epoch 41/50\n",
      "5s - loss: 0.2608 - acc: 0.8929 - val_loss: 0.2124 - val_acc: 0.9217\n",
      "Epoch 42/50\n",
      "5s - loss: 0.2577 - acc: 0.8903 - val_loss: 0.2189 - val_acc: 0.9130\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2640 - acc: 0.8914 - val_loss: 0.2429 - val_acc: 0.8913\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2567 - acc: 0.8939 - val_loss: 0.2033 - val_acc: 0.9217\n",
      "Epoch 45/50\n",
      "4s - loss: 0.2692 - acc: 0.8892 - val_loss: 0.3494 - val_acc: 0.8522\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2674 - acc: 0.8906 - val_loss: 0.2086 - val_acc: 0.9261\n",
      "Epoch 47/50\n",
      "5s - loss: 0.2590 - acc: 0.8945 - val_loss: 0.2015 - val_acc: 0.9304\n",
      "Epoch 48/50\n",
      "4s - loss: 0.2588 - acc: 0.8931 - val_loss: 0.2063 - val_acc: 0.9130\n",
      "Epoch 49/50\n",
      "4s - loss: 0.2595 - acc: 0.8952 - val_loss: 0.2143 - val_acc: 0.9087\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2460 - acc: 0.9010 - val_loss: 0.2671 - val_acc: 0.9000\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.9697607810431306, va score: 0.9736864442746795\n",
      "\n",
      "\n",
      "kfold: 4\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s - loss: 0.6579 - acc: 0.6132 - val_loss: 0.6514 - val_acc: 0.6043\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6387 - acc: 0.6309 - val_loss: 0.6382 - val_acc: 0.6043\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6300 - acc: 0.6285 - val_loss: 0.6197 - val_acc: 0.6043\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6060 - acc: 0.6314 - val_loss: 0.5915 - val_acc: 0.6043\n",
      "Epoch 5/50\n",
      "5s - loss: 0.5733 - acc: 0.6374 - val_loss: 0.5560 - val_acc: 0.6870\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5471 - acc: 0.6847 - val_loss: 0.5218 - val_acc: 0.7043\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5113 - acc: 0.7410 - val_loss: 0.5056 - val_acc: 0.6913\n",
      "Epoch 8/50\n",
      "5s - loss: 0.4997 - acc: 0.7542 - val_loss: 0.4721 - val_acc: 0.8304\n",
      "Epoch 9/50\n",
      "4s - loss: 0.4871 - acc: 0.7576 - val_loss: 0.5301 - val_acc: 0.7348\n",
      "Epoch 10/50\n",
      "5s - loss: 0.4663 - acc: 0.7871 - val_loss: 0.4640 - val_acc: 0.7957\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4404 - acc: 0.7942 - val_loss: 0.4027 - val_acc: 0.8087\n",
      "Epoch 12/50\n",
      "4s - loss: 0.4199 - acc: 0.8072 - val_loss: 0.4199 - val_acc: 0.8087\n",
      "Epoch 13/50\n",
      "5s - loss: 0.4024 - acc: 0.8156 - val_loss: 0.3766 - val_acc: 0.8304\n",
      "Epoch 14/50\n",
      "5s - loss: 0.3980 - acc: 0.8110 - val_loss: 0.3492 - val_acc: 0.8522\n",
      "Epoch 15/50\n",
      "5s - loss: 0.3780 - acc: 0.8354 - val_loss: 0.3653 - val_acc: 0.8391\n",
      "Epoch 16/50\n",
      "5s - loss: 0.3564 - acc: 0.8475 - val_loss: 0.3450 - val_acc: 0.8522\n",
      "Epoch 17/50\n",
      "5s - loss: 0.3532 - acc: 0.8466 - val_loss: 0.3174 - val_acc: 0.8609\n",
      "Epoch 18/50\n",
      "5s - loss: 0.3448 - acc: 0.8541 - val_loss: 0.3018 - val_acc: 0.8696\n",
      "Epoch 19/50\n",
      "4s - loss: 0.3241 - acc: 0.8517 - val_loss: 0.3081 - val_acc: 0.8565\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3242 - acc: 0.8606 - val_loss: 0.2913 - val_acc: 0.8696\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3140 - acc: 0.8692 - val_loss: 0.2765 - val_acc: 0.8739\n",
      "Epoch 22/50\n",
      "5s - loss: 0.2960 - acc: 0.8763 - val_loss: 0.2743 - val_acc: 0.8913\n",
      "Epoch 23/50\n",
      "4s - loss: 0.3017 - acc: 0.8695 - val_loss: 0.2758 - val_acc: 0.8696\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3009 - acc: 0.8743 - val_loss: 0.2910 - val_acc: 0.8696\n",
      "Epoch 25/50\n",
      "5s - loss: 0.3013 - acc: 0.8734 - val_loss: 0.2646 - val_acc: 0.8957\n",
      "Epoch 26/50\n",
      "5s - loss: 0.2955 - acc: 0.8792 - val_loss: 0.2645 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "4s - loss: 0.2821 - acc: 0.8853 - val_loss: 0.3001 - val_acc: 0.8913\n",
      "Epoch 28/50\n",
      "4s - loss: 0.2857 - acc: 0.8836 - val_loss: 0.2691 - val_acc: 0.8783\n",
      "Epoch 29/50\n",
      "4s - loss: 0.2835 - acc: 0.8777 - val_loss: 0.2740 - val_acc: 0.8957\n",
      "Epoch 30/50\n",
      "5s - loss: 0.2742 - acc: 0.8834 - val_loss: 0.2640 - val_acc: 0.8783\n",
      "Epoch 31/50\n",
      "5s - loss: 0.2685 - acc: 0.8848 - val_loss: 0.2528 - val_acc: 0.9043\n",
      "Epoch 32/50\n",
      "5s - loss: 0.2765 - acc: 0.8745 - val_loss: 0.3464 - val_acc: 0.8391\n",
      "Epoch 33/50\n",
      "5s - loss: 0.2827 - acc: 0.8815 - val_loss: 0.2350 - val_acc: 0.9043\n",
      "Epoch 34/50\n",
      "4s - loss: 0.2777 - acc: 0.8828 - val_loss: 0.2370 - val_acc: 0.9217\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2628 - acc: 0.8951 - val_loss: 0.2240 - val_acc: 0.9043\n",
      "Epoch 36/50\n",
      "5s - loss: 0.2572 - acc: 0.8937 - val_loss: 0.2257 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      "4s - loss: 0.2666 - acc: 0.8920 - val_loss: 0.2244 - val_acc: 0.9087\n",
      "Epoch 38/50\n",
      "4s - loss: 0.2689 - acc: 0.8894 - val_loss: 0.2391 - val_acc: 0.8913\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2558 - acc: 0.8966 - val_loss: 0.2118 - val_acc: 0.9174\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2668 - acc: 0.8887 - val_loss: 0.2916 - val_acc: 0.8957\n",
      "Epoch 41/50\n",
      "4s - loss: 0.2643 - acc: 0.8915 - val_loss: 0.2509 - val_acc: 0.8826\n",
      "Epoch 42/50\n",
      "5s - loss: 0.2627 - acc: 0.8919 - val_loss: 0.2072 - val_acc: 0.9087\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2612 - acc: 0.8936 - val_loss: 0.2141 - val_acc: 0.9043\n",
      "Epoch 44/50\n",
      "4s - loss: 0.2554 - acc: 0.8999 - val_loss: 0.2153 - val_acc: 0.9043\n",
      "Epoch 45/50\n",
      "5s - loss: 0.2498 - acc: 0.8924 - val_loss: 0.2072 - val_acc: 0.9130\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2456 - acc: 0.8994 - val_loss: 0.2739 - val_acc: 0.8826\n",
      "Epoch 47/50\n",
      "5s - loss: 0.2426 - acc: 0.9032 - val_loss: 0.2046 - val_acc: 0.9087\n",
      "Epoch 48/50\n",
      "4s - loss: 0.2439 - acc: 0.9025 - val_loss: 0.2179 - val_acc: 0.9043\n",
      "Epoch 49/50\n",
      "5s - loss: 0.2464 - acc: 0.9004 - val_loss: 0.2156 - val_acc: 0.9174\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2407 - acc: 0.9012 - val_loss: 0.3148 - val_acc: 0.8609\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.9721179276282504, va score: 0.9746224998023558\n",
      "\n",
      "\n",
      "kfold: 5\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6606 - acc: 0.6100 - val_loss: 0.5999 - val_acc: 0.6900\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6446 - acc: 0.6293 - val_loss: 0.6008 - val_acc: 0.6900\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6356 - acc: 0.6304 - val_loss: 0.5854 - val_acc: 0.6900\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6248 - acc: 0.6301 - val_loss: 0.5796 - val_acc: 0.6900\n",
      "Epoch 5/50\n",
      "5s - loss: 0.6047 - acc: 0.6309 - val_loss: 0.5568 - val_acc: 0.6900\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5825 - acc: 0.6480 - val_loss: 0.5327 - val_acc: 0.6943\n",
      "Epoch 7/50\n",
      "4s - loss: 0.5696 - acc: 0.6798 - val_loss: 0.6426 - val_acc: 0.6638\n",
      "Epoch 8/50\n",
      "5s - loss: 0.5550 - acc: 0.6877 - val_loss: 0.5120 - val_acc: 0.7293\n",
      "Epoch 9/50\n",
      "4s - loss: 0.5215 - acc: 0.7471 - val_loss: 0.5715 - val_acc: 0.6812\n",
      "Epoch 10/50\n",
      "5s - loss: 0.5066 - acc: 0.7394 - val_loss: 0.4479 - val_acc: 0.7555\n",
      "Epoch 11/50\n",
      "4s - loss: 0.4926 - acc: 0.7505 - val_loss: 0.4552 - val_acc: 0.8035\n",
      "Epoch 12/50\n",
      "5s - loss: 0.4771 - acc: 0.7645 - val_loss: 0.4303 - val_acc: 0.7686\n",
      "Epoch 13/50\n",
      "5s - loss: 0.4490 - acc: 0.7901 - val_loss: 0.3912 - val_acc: 0.7729\n",
      "Epoch 14/50\n",
      "4s - loss: 0.4341 - acc: 0.7915 - val_loss: 0.4254 - val_acc: 0.7817\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4173 - acc: 0.8087 - val_loss: 0.3553 - val_acc: 0.7991\n",
      "Epoch 16/50\n",
      "5s - loss: 0.4060 - acc: 0.8045 - val_loss: 0.3503 - val_acc: 0.8428\n",
      "Epoch 17/50\n",
      "4s - loss: 0.3832 - acc: 0.8279 - val_loss: 0.4219 - val_acc: 0.8035\n",
      "Epoch 18/50\n",
      "5s - loss: 0.3830 - acc: 0.8263 - val_loss: 0.3404 - val_acc: 0.8166\n",
      "Epoch 19/50\n",
      "4s - loss: 0.3646 - acc: 0.8332 - val_loss: 0.3924 - val_acc: 0.7991\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3597 - acc: 0.8384 - val_loss: 0.3009 - val_acc: 0.8690\n",
      "Epoch 21/50\n",
      "4s - loss: 0.3434 - acc: 0.8467 - val_loss: 0.4065 - val_acc: 0.7948\n",
      "Epoch 22/50\n",
      "5s - loss: 0.3270 - acc: 0.8580 - val_loss: 0.2867 - val_acc: 0.8515\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3290 - acc: 0.8545 - val_loss: 0.2585 - val_acc: 0.8821\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3201 - acc: 0.8583 - val_loss: 0.2737 - val_acc: 0.8777\n",
      "Epoch 25/50\n",
      "4s - loss: 0.3052 - acc: 0.8733 - val_loss: 0.2651 - val_acc: 0.8952\n",
      "Epoch 26/50\n",
      "5s - loss: 0.3098 - acc: 0.8678 - val_loss: 0.2464 - val_acc: 0.8952\n",
      "Epoch 27/50\n",
      "4s - loss: 0.3106 - acc: 0.8724 - val_loss: 0.2627 - val_acc: 0.8865\n",
      "Epoch 28/50\n",
      "5s - loss: 0.3031 - acc: 0.8779 - val_loss: 0.2439 - val_acc: 0.8908\n",
      "Epoch 29/50\n",
      "4s - loss: 0.2977 - acc: 0.8764 - val_loss: 0.2596 - val_acc: 0.8908\n",
      "Epoch 30/50\n",
      "5s - loss: 0.2905 - acc: 0.8752 - val_loss: 0.2312 - val_acc: 0.8952\n",
      "Epoch 31/50\n",
      "5s - loss: 0.2927 - acc: 0.8724 - val_loss: 0.2289 - val_acc: 0.8821\n",
      "Epoch 32/50\n",
      "4s - loss: 0.2716 - acc: 0.8865 - val_loss: 0.2562 - val_acc: 0.8865\n",
      "Epoch 33/50\n",
      "4s - loss: 0.2866 - acc: 0.8851 - val_loss: 0.2338 - val_acc: 0.8908\n",
      "Epoch 34/50\n",
      "5s - loss: 0.2725 - acc: 0.8873 - val_loss: 0.2254 - val_acc: 0.8952\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2754 - acc: 0.8895 - val_loss: 0.2234 - val_acc: 0.9127\n",
      "Epoch 36/50\n",
      "4s - loss: 0.2741 - acc: 0.8882 - val_loss: 0.3561 - val_acc: 0.8603\n",
      "Epoch 37/50\n",
      "5s - loss: 0.2680 - acc: 0.8856 - val_loss: 0.2938 - val_acc: 0.8734\n",
      "Epoch 38/50\n",
      "4s - loss: 0.2839 - acc: 0.8853 - val_loss: 0.2861 - val_acc: 0.8690\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2694 - acc: 0.8934 - val_loss: 0.2145 - val_acc: 0.8996\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2466 - acc: 0.8974 - val_loss: 0.2503 - val_acc: 0.8952\n",
      "Epoch 41/50\n",
      "5s - loss: 0.2524 - acc: 0.8986 - val_loss: 0.2204 - val_acc: 0.9083\n",
      "Epoch 42/50\n",
      "5s - loss: 0.2514 - acc: 0.8965 - val_loss: 0.2088 - val_acc: 0.9083\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2549 - acc: 0.8978 - val_loss: 0.2185 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2533 - acc: 0.9027 - val_loss: 0.1993 - val_acc: 0.9170\n",
      "Epoch 45/50\n",
      "5s - loss: 0.2522 - acc: 0.9024 - val_loss: 0.2103 - val_acc: 0.9258\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2483 - acc: 0.9025 - val_loss: 0.2002 - val_acc: 0.9170\n",
      "Epoch 47/50\n",
      "5s - loss: 0.2513 - acc: 0.8995 - val_loss: 0.2511 - val_acc: 0.8996\n",
      "Epoch 48/50\n",
      "4s - loss: 0.2528 - acc: 0.9025 - val_loss: 0.2082 - val_acc: 0.9083\n",
      "Epoch 49/50\n",
      "5s - loss: 0.2434 - acc: 0.9041 - val_loss: 0.2190 - val_acc: 0.9170\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2481 - acc: 0.8998 - val_loss: 0.1994 - val_acc: 0.9083\n",
      "Epoch 00047: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr score: 0.9748936727709626, va score: 0.9712069887680513\n",
      "\n",
      "\n",
      "kfold: 6\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6583 - acc: 0.6102 - val_loss: 0.6357 - val_acc: 0.6419\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6422 - acc: 0.6296 - val_loss: 0.6241 - val_acc: 0.6419\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6349 - acc: 0.6277 - val_loss: 0.6091 - val_acc: 0.6419\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6205 - acc: 0.6301 - val_loss: 0.5871 - val_acc: 0.6419\n",
      "Epoch 5/50\n",
      "5s - loss: 0.5974 - acc: 0.6306 - val_loss: 0.5518 - val_acc: 0.6376\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5707 - acc: 0.6458 - val_loss: 0.5190 - val_acc: 0.6550\n",
      "Epoch 7/50\n",
      "4s - loss: 0.5482 - acc: 0.6826 - val_loss: 0.6398 - val_acc: 0.6332\n",
      "Epoch 8/50\n",
      "5s - loss: 0.5352 - acc: 0.7148 - val_loss: 0.4905 - val_acc: 0.7860\n",
      "Epoch 9/50\n",
      "5s - loss: 0.5137 - acc: 0.7380 - val_loss: 0.4612 - val_acc: 0.7860\n",
      "Epoch 10/50\n",
      "5s - loss: 0.4914 - acc: 0.7410 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4853 - acc: 0.7597 - val_loss: 0.4182 - val_acc: 0.7991\n",
      "Epoch 12/50\n",
      "4s - loss: 0.4673 - acc: 0.7797 - val_loss: 0.4432 - val_acc: 0.7991\n",
      "Epoch 13/50\n",
      "5s - loss: 0.4500 - acc: 0.7847 - val_loss: 0.3736 - val_acc: 0.8166\n",
      "Epoch 14/50\n",
      "5s - loss: 0.4445 - acc: 0.7959 - val_loss: 0.3790 - val_acc: 0.8297\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4384 - acc: 0.7886 - val_loss: 0.3525 - val_acc: 0.8341\n",
      "Epoch 16/50\n",
      "4s - loss: 0.4185 - acc: 0.8100 - val_loss: 0.3593 - val_acc: 0.8079\n",
      "Epoch 17/50\n",
      "5s - loss: 0.4064 - acc: 0.8084 - val_loss: 0.3989 - val_acc: 0.7948\n",
      "Epoch 18/50\n",
      "5s - loss: 0.3971 - acc: 0.8064 - val_loss: 0.3593 - val_acc: 0.8297\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3932 - acc: 0.8273 - val_loss: 0.3936 - val_acc: 0.8166\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3785 - acc: 0.8236 - val_loss: 0.3329 - val_acc: 0.8253\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3699 - acc: 0.8281 - val_loss: 0.3168 - val_acc: 0.8472\n",
      "Epoch 22/50\n",
      "5s - loss: 0.3607 - acc: 0.8470 - val_loss: 0.3045 - val_acc: 0.8559\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3471 - acc: 0.8460 - val_loss: 0.3041 - val_acc: 0.8559\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3397 - acc: 0.8484 - val_loss: 0.2789 - val_acc: 0.8821\n",
      "Epoch 25/50\n",
      "4s - loss: 0.3193 - acc: 0.8630 - val_loss: 0.2844 - val_acc: 0.8690\n",
      "Epoch 26/50\n",
      "4s - loss: 0.3286 - acc: 0.8529 - val_loss: 0.2995 - val_acc: 0.8734\n",
      "Epoch 27/50\n",
      "4s - loss: 0.3211 - acc: 0.8602 - val_loss: 0.2884 - val_acc: 0.8603\n",
      "Epoch 28/50\n",
      "5s - loss: 0.3109 - acc: 0.8648 - val_loss: 0.2738 - val_acc: 0.8559\n",
      "Epoch 29/50\n",
      "5s - loss: 0.3141 - acc: 0.8638 - val_loss: 0.2767 - val_acc: 0.8777\n",
      "Epoch 30/50\n",
      "4s - loss: 0.2946 - acc: 0.8692 - val_loss: 0.2774 - val_acc: 0.8603\n",
      "Epoch 31/50\n",
      "4s - loss: 0.3024 - acc: 0.8723 - val_loss: 0.2880 - val_acc: 0.8734\n",
      "Epoch 32/50\n",
      "4s - loss: 0.2895 - acc: 0.8798 - val_loss: 0.2909 - val_acc: 0.8690\n",
      "Epoch 33/50\n",
      "5s - loss: 0.2868 - acc: 0.8797 - val_loss: 0.2495 - val_acc: 0.8821\n",
      "Epoch 34/50\n",
      "4s - loss: 0.2813 - acc: 0.8823 - val_loss: 0.2542 - val_acc: 0.8821\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2799 - acc: 0.8818 - val_loss: 0.2419 - val_acc: 0.8865\n",
      "Epoch 36/50\n",
      "4s - loss: 0.2802 - acc: 0.8802 - val_loss: 0.2670 - val_acc: 0.8646\n",
      "Epoch 37/50\n",
      "4s - loss: 0.2688 - acc: 0.8862 - val_loss: 0.2783 - val_acc: 0.8734\n",
      "Epoch 38/50\n",
      "4s - loss: 0.2754 - acc: 0.8784 - val_loss: 0.2441 - val_acc: 0.8777\n",
      "Epoch 39/50\n",
      "4s - loss: 0.2695 - acc: 0.8924 - val_loss: 0.2593 - val_acc: 0.8821\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2624 - acc: 0.8956 - val_loss: 0.2445 - val_acc: 0.8908\n",
      "Epoch 41/50\n",
      "5s - loss: 0.2602 - acc: 0.8991 - val_loss: 0.2375 - val_acc: 0.8908\n",
      "Epoch 42/50\n",
      "4s - loss: 0.2523 - acc: 0.8931 - val_loss: 0.2822 - val_acc: 0.8690\n",
      "Epoch 43/50\n",
      "5s - loss: 0.2625 - acc: 0.8966 - val_loss: 0.2425 - val_acc: 0.8952\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2604 - acc: 0.8937 - val_loss: 0.2265 - val_acc: 0.9039\n",
      "Epoch 45/50\n",
      "5s - loss: 0.2519 - acc: 0.9007 - val_loss: 0.2253 - val_acc: 0.8908\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2475 - acc: 0.8980 - val_loss: 0.2532 - val_acc: 0.8996\n",
      "Epoch 47/50\n",
      "5s - loss: 0.2359 - acc: 0.9067 - val_loss: 0.2136 - val_acc: 0.9083\n",
      "Epoch 48/50\n",
      "4s - loss: 0.2460 - acc: 0.8953 - val_loss: 0.2294 - val_acc: 0.9039\n",
      "Epoch 49/50\n",
      "5s - loss: 0.2417 - acc: 0.8968 - val_loss: 0.2105 - val_acc: 0.8952\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2431 - acc: 0.9074 - val_loss: 0.2152 - val_acc: 0.8996\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.9754922793596827, va score: 0.9671478347436535\n",
      "\n",
      "\n",
      "kfold: 7\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6710 - acc: 0.5897 - val_loss: 0.6811 - val_acc: 0.5590\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6489 - acc: 0.6277 - val_loss: 0.6786 - val_acc: 0.5590\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6435 - acc: 0.6293 - val_loss: 0.6696 - val_acc: 0.5590\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6367 - acc: 0.6293 - val_loss: 0.6611 - val_acc: 0.5590\n",
      "Epoch 5/50\n",
      "4s - loss: 0.6249 - acc: 0.6324 - val_loss: 0.6665 - val_acc: 0.5590\n",
      "Epoch 6/50\n",
      "5s - loss: 0.6068 - acc: 0.6295 - val_loss: 0.6176 - val_acc: 0.5590\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5820 - acc: 0.6359 - val_loss: 0.5853 - val_acc: 0.6638\n",
      "Epoch 8/50\n",
      "4s - loss: 0.5547 - acc: 0.6806 - val_loss: 0.6028 - val_acc: 0.6157\n",
      "Epoch 9/50\n",
      "5s - loss: 0.5251 - acc: 0.7179 - val_loss: 0.5319 - val_acc: 0.7249\n",
      "Epoch 10/50\n",
      "4s - loss: 0.5148 - acc: 0.7301 - val_loss: 0.6646 - val_acc: 0.6376\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4940 - acc: 0.7573 - val_loss: 0.4762 - val_acc: 0.7860\n",
      "Epoch 12/50\n",
      "4s - loss: 0.4966 - acc: 0.7446 - val_loss: 0.6027 - val_acc: 0.6638\n",
      "Epoch 13/50\n",
      "5s - loss: 0.4572 - acc: 0.7840 - val_loss: 0.4357 - val_acc: 0.8253\n",
      "Epoch 14/50\n",
      "4s - loss: 0.4305 - acc: 0.8043 - val_loss: 0.4590 - val_acc: 0.7904\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4316 - acc: 0.7934 - val_loss: 0.3914 - val_acc: 0.8297\n",
      "Epoch 16/50\n",
      "4s - loss: 0.4179 - acc: 0.8066 - val_loss: 0.4528 - val_acc: 0.7948\n",
      "Epoch 17/50\n",
      "5s - loss: 0.3843 - acc: 0.8272 - val_loss: 0.3583 - val_acc: 0.8472\n",
      "Epoch 18/50\n",
      "5s - loss: 0.3809 - acc: 0.8173 - val_loss: 0.3403 - val_acc: 0.8559\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3610 - acc: 0.8348 - val_loss: 0.3247 - val_acc: 0.8646\n",
      "Epoch 20/50\n",
      "4s - loss: 0.3490 - acc: 0.8407 - val_loss: 0.3775 - val_acc: 0.8472\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3547 - acc: 0.8410 - val_loss: 0.3039 - val_acc: 0.8734\n",
      "Epoch 22/50\n",
      "4s - loss: 0.3470 - acc: 0.8423 - val_loss: 0.3064 - val_acc: 0.8734\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3270 - acc: 0.8572 - val_loss: 0.2947 - val_acc: 0.8952\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3205 - acc: 0.8594 - val_loss: 0.2686 - val_acc: 0.8865\n",
      "Epoch 25/50\n",
      "4s - loss: 0.3230 - acc: 0.8625 - val_loss: 0.3693 - val_acc: 0.8690\n",
      "Epoch 26/50\n",
      "4s - loss: 0.3051 - acc: 0.8686 - val_loss: 0.3494 - val_acc: 0.8952\n",
      "Epoch 27/50\n",
      "4s - loss: 0.3215 - acc: 0.8581 - val_loss: 0.3261 - val_acc: 0.8908\n",
      "Epoch 28/50\n",
      "4s - loss: 0.2886 - acc: 0.8753 - val_loss: 0.2752 - val_acc: 0.9170\n",
      "Epoch 29/50\n",
      "5s - loss: 0.2735 - acc: 0.8817 - val_loss: 0.2860 - val_acc: 0.9127\n",
      "Epoch 30/50\n",
      "5s - loss: 0.2856 - acc: 0.8829 - val_loss: 0.2644 - val_acc: 0.9170\n",
      "Epoch 31/50\n",
      "5s - loss: 0.2972 - acc: 0.8656 - val_loss: 0.2353 - val_acc: 0.9214\n",
      "Epoch 32/50\n",
      "4s - loss: 0.2783 - acc: 0.8856 - val_loss: 0.3940 - val_acc: 0.8908\n",
      "Epoch 33/50\n",
      "4s - loss: 0.2685 - acc: 0.8898 - val_loss: 0.2504 - val_acc: 0.9039\n",
      "Epoch 34/50\n",
      "5s - loss: 0.2716 - acc: 0.8857 - val_loss: 0.3344 - val_acc: 0.8952\n",
      "Epoch 35/50\n",
      "4s - loss: 0.2715 - acc: 0.8848 - val_loss: 0.2910 - val_acc: 0.9258\n",
      "Epoch 36/50\n",
      "5s - loss: 0.2625 - acc: 0.8871 - val_loss: 0.2748 - val_acc: 0.9301\n",
      "Epoch 37/50\n",
      "5s - loss: 0.2622 - acc: 0.8934 - val_loss: 0.2246 - val_acc: 0.9170\n",
      "Epoch 38/50\n",
      "5s - loss: 0.2477 - acc: 0.8965 - val_loss: 0.2623 - val_acc: 0.9432\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2442 - acc: 0.9007 - val_loss: 0.2168 - val_acc: 0.9083\n",
      "Epoch 40/50\n",
      "4s - loss: 0.2605 - acc: 0.8950 - val_loss: 0.2233 - val_acc: 0.9127\n",
      "Epoch 41/50\n",
      "4s - loss: 0.2454 - acc: 0.8988 - val_loss: 0.2374 - val_acc: 0.9170\n",
      "Epoch 42/50\n",
      "5s - loss: 0.2675 - acc: 0.8910 - val_loss: 0.2315 - val_acc: 0.9345\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2408 - acc: 0.8995 - val_loss: 0.2324 - val_acc: 0.9214\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2349 - acc: 0.9066 - val_loss: 0.2096 - val_acc: 0.9127\n",
      "Epoch 45/50\n",
      "4s - loss: 0.2376 - acc: 0.9066 - val_loss: 0.2098 - val_acc: 0.9127\n",
      "Epoch 46/50\n",
      "5s - loss: 0.2353 - acc: 0.9044 - val_loss: 0.2834 - val_acc: 0.9301\n",
      "Epoch 47/50\n",
      "4s - loss: 0.2454 - acc: 0.8978 - val_loss: 0.2116 - val_acc: 0.9127\n",
      "Epoch 48/50\n",
      "5s - loss: 0.2522 - acc: 0.9019 - val_loss: 0.2116 - val_acc: 0.9258\n",
      "Epoch 49/50\n",
      "5s - loss: 0.2313 - acc: 0.9093 - val_loss: 0.2538 - val_acc: 0.9301\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2418 - acc: 0.9043 - val_loss: 0.2153 - val_acc: 0.9170\n",
      "Epoch 00047: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr score: 0.9804174695131369, va score: 0.9774133663366336\n",
      "\n",
      "\n",
      "kfold: 8\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6570 - acc: 0.6141 - val_loss: 0.6432 - val_acc: 0.6332\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6449 - acc: 0.6296 - val_loss: 0.6307 - val_acc: 0.6332\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6386 - acc: 0.6304 - val_loss: 0.6182 - val_acc: 0.6332\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6239 - acc: 0.6311 - val_loss: 0.5993 - val_acc: 0.6332\n",
      "Epoch 5/50\n",
      "5s - loss: 0.6008 - acc: 0.6329 - val_loss: 0.5691 - val_acc: 0.6332\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5794 - acc: 0.6586 - val_loss: 0.5345 - val_acc: 0.6987\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5633 - acc: 0.6773 - val_loss: 0.5280 - val_acc: 0.8035\n",
      "Epoch 8/50\n",
      "4s - loss: 0.5393 - acc: 0.7231 - val_loss: 0.7618 - val_acc: 0.3668\n",
      "Epoch 9/50\n",
      "5s - loss: 0.6568 - acc: 0.5946 - val_loss: 0.6209 - val_acc: 0.6419\n",
      "Epoch 10/50\n",
      "4s - loss: 0.6145 - acc: 0.6397 - val_loss: 0.5759 - val_acc: 0.6507\n",
      "Epoch 11/50\n",
      "4s - loss: 0.5824 - acc: 0.6513 - val_loss: 0.5754 - val_acc: 0.7686\n",
      "Epoch 12/50\n",
      "4s - loss: 0.5514 - acc: 0.6998 - val_loss: 0.5537 - val_acc: 0.7511\n",
      "Epoch 13/50\n",
      "5s - loss: 0.5296 - acc: 0.7182 - val_loss: 0.4783 - val_acc: 0.7642\n",
      "Epoch 14/50\n",
      "4s - loss: 0.5332 - acc: 0.7102 - val_loss: 0.4923 - val_acc: 0.7511\n",
      "Epoch 15/50\n",
      "5s - loss: 0.5001 - acc: 0.7573 - val_loss: 0.4675 - val_acc: 0.7380\n",
      "Epoch 16/50\n",
      "5s - loss: 0.4806 - acc: 0.7645 - val_loss: 0.4256 - val_acc: 0.7991\n",
      "Epoch 17/50\n",
      "5s - loss: 0.4490 - acc: 0.7848 - val_loss: 0.5002 - val_acc: 0.7424\n",
      "Epoch 18/50\n",
      "5s - loss: 0.4147 - acc: 0.7996 - val_loss: 0.3747 - val_acc: 0.8341\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3933 - acc: 0.8189 - val_loss: 0.3721 - val_acc: 0.7991\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3685 - acc: 0.8372 - val_loss: 0.3376 - val_acc: 0.8559\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3501 - acc: 0.8453 - val_loss: 0.3178 - val_acc: 0.8603\n",
      "Epoch 22/50\n",
      "4s - loss: 0.3375 - acc: 0.8551 - val_loss: 0.3315 - val_acc: 0.8559\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3356 - acc: 0.8441 - val_loss: 0.3111 - val_acc: 0.8646\n",
      "Epoch 24/50\n",
      "4s - loss: 0.3190 - acc: 0.8647 - val_loss: 0.3570 - val_acc: 0.8341\n",
      "Epoch 25/50\n",
      "5s - loss: 0.3092 - acc: 0.8649 - val_loss: 0.2708 - val_acc: 0.8865\n",
      "Epoch 26/50\n",
      "4s - loss: 0.3090 - acc: 0.8716 - val_loss: 0.2736 - val_acc: 0.8690\n",
      "Epoch 27/50\n",
      "4s - loss: 0.3113 - acc: 0.8575 - val_loss: 0.2715 - val_acc: 0.8777\n",
      "Epoch 28/50\n",
      "4s - loss: 0.3119 - acc: 0.8664 - val_loss: 0.2778 - val_acc: 0.8734\n",
      "Epoch 29/50\n",
      "5s - loss: 0.2995 - acc: 0.8743 - val_loss: 0.2533 - val_acc: 0.8865\n",
      "Epoch 30/50\n",
      "4s - loss: 0.3007 - acc: 0.8715 - val_loss: 0.2539 - val_acc: 0.8777\n",
      "Epoch 31/50\n",
      "5s - loss: 0.2833 - acc: 0.8763 - val_loss: 0.2506 - val_acc: 0.8952\n",
      "Epoch 32/50\n",
      "4s - loss: 0.2986 - acc: 0.8695 - val_loss: 0.2510 - val_acc: 0.8865\n",
      "Epoch 33/50\n",
      "4s - loss: 0.2938 - acc: 0.8690 - val_loss: 0.2573 - val_acc: 0.8821\n",
      "Epoch 34/50\n",
      "5s - loss: 0.2932 - acc: 0.8794 - val_loss: 0.2494 - val_acc: 0.8821\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2857 - acc: 0.8732 - val_loss: 0.2334 - val_acc: 0.9039\n",
      "Epoch 36/50\n",
      "5s - loss: 0.2943 - acc: 0.8687 - val_loss: 0.2628 - val_acc: 0.8777\n",
      "Epoch 37/50\n",
      "4s - loss: 0.2807 - acc: 0.8812 - val_loss: 0.2390 - val_acc: 0.8952\n",
      "Epoch 38/50\n",
      "5s - loss: 0.2867 - acc: 0.8741 - val_loss: 0.2318 - val_acc: 0.8952\n",
      "Epoch 39/50\n",
      "4s - loss: 0.2684 - acc: 0.8856 - val_loss: 0.2443 - val_acc: 0.8996\n",
      "Epoch 40/50\n",
      "5s - loss: 0.2728 - acc: 0.8802 - val_loss: 0.3277 - val_acc: 0.8384\n",
      "Epoch 41/50\n",
      "5s - loss: 0.2763 - acc: 0.8849 - val_loss: 0.2248 - val_acc: 0.9127\n",
      "Epoch 42/50\n",
      "4s - loss: 0.2648 - acc: 0.8890 - val_loss: 0.2672 - val_acc: 0.8908\n",
      "Epoch 43/50\n",
      "5s - loss: 0.2707 - acc: 0.8951 - val_loss: 0.2228 - val_acc: 0.8996\n",
      "Epoch 44/50\n",
      "4s - loss: 0.2574 - acc: 0.8932 - val_loss: 0.2342 - val_acc: 0.8952\n",
      "Epoch 45/50\n",
      "4s - loss: 0.2569 - acc: 0.8964 - val_loss: 0.2290 - val_acc: 0.8952\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2604 - acc: 0.8934 - val_loss: 0.2268 - val_acc: 0.9039\n",
      "Epoch 47/50\n",
      "4s - loss: 0.2567 - acc: 0.8929 - val_loss: 0.2480 - val_acc: 0.9039\n",
      "Epoch 48/50\n",
      "5s - loss: 0.2622 - acc: 0.8890 - val_loss: 0.2153 - val_acc: 0.9039\n",
      "Epoch 49/50\n",
      "5s - loss: 0.2479 - acc: 0.8941 - val_loss: 0.2038 - val_acc: 0.9214\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2590 - acc: 0.8892 - val_loss: 0.2173 - val_acc: 0.9301\n",
      "Epoch 00047: early stopping\n",
      "tr score: 0.9731276768206109, va score: 0.9774220032840722\n",
      "\n",
      "\n",
      "kfold: 9\n",
      "Epoch 1/50\n",
      "5s - loss: 0.6683 - acc: 0.6151 - val_loss: 0.6338 - val_acc: 0.6507\n",
      "Epoch 2/50\n",
      "4s - loss: 0.6472 - acc: 0.6317 - val_loss: 0.6231 - val_acc: 0.6507\n",
      "Epoch 3/50\n",
      "5s - loss: 0.6358 - acc: 0.6301 - val_loss: 0.6121 - val_acc: 0.6507\n",
      "Epoch 4/50\n",
      "5s - loss: 0.6235 - acc: 0.6324 - val_loss: 0.5866 - val_acc: 0.6507\n",
      "Epoch 5/50\n",
      "5s - loss: 0.6025 - acc: 0.6293 - val_loss: 0.5615 - val_acc: 0.6507\n",
      "Epoch 6/50\n",
      "5s - loss: 0.5779 - acc: 0.6429 - val_loss: 0.5253 - val_acc: 0.6987\n",
      "Epoch 7/50\n",
      "5s - loss: 0.5505 - acc: 0.6831 - val_loss: 0.5123 - val_acc: 0.7991\n",
      "Epoch 8/50\n",
      "5s - loss: 0.5365 - acc: 0.7153 - val_loss: 0.5147 - val_acc: 0.6856\n",
      "Epoch 9/50\n",
      "5s - loss: 0.5197 - acc: 0.7375 - val_loss: 0.4444 - val_acc: 0.8341\n",
      "Epoch 10/50\n",
      "5s - loss: 0.5175 - acc: 0.7450 - val_loss: 0.4179 - val_acc: 0.8297\n",
      "Epoch 11/50\n",
      "5s - loss: 0.4759 - acc: 0.7774 - val_loss: 0.4265 - val_acc: 0.8472\n",
      "Epoch 12/50\n",
      "4s - loss: 0.4623 - acc: 0.7856 - val_loss: 0.5142 - val_acc: 0.7467\n",
      "Epoch 13/50\n",
      "5s - loss: 0.4579 - acc: 0.7837 - val_loss: 0.3541 - val_acc: 0.8603\n",
      "Epoch 14/50\n",
      "4s - loss: 0.4292 - acc: 0.7951 - val_loss: 0.3700 - val_acc: 0.8428\n",
      "Epoch 15/50\n",
      "5s - loss: 0.4230 - acc: 0.7995 - val_loss: 0.3190 - val_acc: 0.8952\n",
      "Epoch 16/50\n",
      "5s - loss: 0.4057 - acc: 0.8175 - val_loss: 0.2923 - val_acc: 0.8996\n",
      "Epoch 17/50\n",
      "4s - loss: 0.3936 - acc: 0.8191 - val_loss: 0.3434 - val_acc: 0.8603\n",
      "Epoch 18/50\n",
      "4s - loss: 0.3883 - acc: 0.8270 - val_loss: 0.4042 - val_acc: 0.7948\n",
      "Epoch 19/50\n",
      "5s - loss: 0.3581 - acc: 0.8458 - val_loss: 0.2922 - val_acc: 0.8865\n",
      "Epoch 20/50\n",
      "5s - loss: 0.3471 - acc: 0.8524 - val_loss: 0.2614 - val_acc: 0.8996\n",
      "Epoch 21/50\n",
      "5s - loss: 0.3325 - acc: 0.8610 - val_loss: 0.2191 - val_acc: 0.9214\n",
      "Epoch 22/50\n",
      "4s - loss: 0.3144 - acc: 0.8663 - val_loss: 0.2224 - val_acc: 0.9170\n",
      "Epoch 23/50\n",
      "5s - loss: 0.3143 - acc: 0.8555 - val_loss: 0.2105 - val_acc: 0.9476\n",
      "Epoch 24/50\n",
      "5s - loss: 0.3252 - acc: 0.8556 - val_loss: 0.1938 - val_acc: 0.9301\n",
      "Epoch 25/50\n",
      "5s - loss: 0.3066 - acc: 0.8674 - val_loss: 0.1986 - val_acc: 0.9214\n",
      "Epoch 26/50\n",
      "5s - loss: 0.3084 - acc: 0.8679 - val_loss: 0.2892 - val_acc: 0.8646\n",
      "Epoch 27/50\n",
      "4s - loss: 0.3083 - acc: 0.8713 - val_loss: 0.1953 - val_acc: 0.9432\n",
      "Epoch 28/50\n",
      "4s - loss: 0.3099 - acc: 0.8691 - val_loss: 0.2513 - val_acc: 0.8952\n",
      "Epoch 29/50\n",
      "5s - loss: 0.2888 - acc: 0.8784 - val_loss: 0.1789 - val_acc: 0.9520\n",
      "Epoch 30/50\n",
      "5s - loss: 0.2904 - acc: 0.8785 - val_loss: 0.1869 - val_acc: 0.9389\n",
      "Epoch 31/50\n",
      "4s - loss: 0.3015 - acc: 0.8712 - val_loss: 0.2103 - val_acc: 0.9301\n",
      "Epoch 32/50\n",
      "5s - loss: 0.2929 - acc: 0.8707 - val_loss: 0.1887 - val_acc: 0.9127\n",
      "Epoch 33/50\n",
      "4s - loss: 0.2915 - acc: 0.8729 - val_loss: 0.2107 - val_acc: 0.9127\n",
      "Epoch 34/50\n",
      "5s - loss: 0.2890 - acc: 0.8782 - val_loss: 0.1767 - val_acc: 0.9432\n",
      "Epoch 35/50\n",
      "5s - loss: 0.2809 - acc: 0.8803 - val_loss: 0.1712 - val_acc: 0.9476\n",
      "Epoch 36/50\n",
      "5s - loss: 0.2634 - acc: 0.8925 - val_loss: 0.1688 - val_acc: 0.9301\n",
      "Epoch 37/50\n",
      "5s - loss: 0.2733 - acc: 0.8904 - val_loss: 0.1637 - val_acc: 0.9301\n",
      "Epoch 38/50\n",
      "5s - loss: 0.2815 - acc: 0.8841 - val_loss: 0.1624 - val_acc: 0.9432\n",
      "Epoch 39/50\n",
      "5s - loss: 0.2590 - acc: 0.8984 - val_loss: 0.1960 - val_acc: 0.9039\n",
      "Epoch 40/50\n",
      "5s - loss: 0.2667 - acc: 0.8929 - val_loss: 0.1562 - val_acc: 0.9607\n",
      "Epoch 41/50\n",
      "5s - loss: 0.2664 - acc: 0.8877 - val_loss: 0.1503 - val_acc: 0.9563\n",
      "Epoch 42/50\n",
      "5s - loss: 0.2624 - acc: 0.8920 - val_loss: 0.1762 - val_acc: 0.9301\n",
      "Epoch 43/50\n",
      "4s - loss: 0.2582 - acc: 0.8893 - val_loss: 0.1896 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      "5s - loss: 0.2659 - acc: 0.8859 - val_loss: 0.1809 - val_acc: 0.9127\n",
      "Epoch 45/50\n",
      "5s - loss: 0.2709 - acc: 0.8881 - val_loss: 0.1500 - val_acc: 0.9432\n",
      "Epoch 46/50\n",
      "4s - loss: 0.2664 - acc: 0.8892 - val_loss: 0.1860 - val_acc: 0.9520\n",
      "Epoch 47/50\n",
      "4s - loss: 0.2656 - acc: 0.8850 - val_loss: 0.1866 - val_acc: 0.9520\n",
      "Epoch 48/50\n",
      "5s - loss: 0.2570 - acc: 0.8983 - val_loss: 0.1662 - val_acc: 0.9301\n",
      "Epoch 49/50\n",
      "4s - loss: 0.2634 - acc: 0.8980 - val_loss: 0.1597 - val_acc: 0.9607\n",
      "Epoch 50/50\n",
      "4s - loss: 0.2606 - acc: 0.8919 - val_loss: 0.1672 - val_acc: 0.9520\n",
      "Epoch 00047: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr score: 0.9705745451601805, va score: 0.984731543624161\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ok now lets try a k folds\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "\n",
    "kf = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "score_func = metrics.roc_auc_score\n",
    "\n",
    "i = 0\n",
    "va_score_per_model = {i: 0 for i in range(10)}\n",
    "\n",
    "for rain_ixs, valid_ixs in kf.split(train_img):\n",
    "    x_train = train_img[train_ixs]\n",
    "    x_valid = train_img[valid_ixs]\n",
    "    y_train = train_label[train_ixs]\n",
    "    y_valid = train_label[valid_ixs]\n",
    "\n",
    "    gen = ImageDataGenerator(\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        fill_mode = 'nearest')\n",
    "\n",
    "    # only required of featurewise center or zca whitening or a few other things\n",
    "    # gen.fit(x_train)\n",
    "\n",
    "    model = make_model((128,128,3), grab_optimizer('sgd', 0.005))\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint('/scratch/yns207/data_invasive/model_jun21_kfold_'+str(i)+'.model', \n",
    "                                        monitor='val_loss', \n",
    "                                        save_best_only=True)\n",
    "    \n",
    "    print('kfold: {}'.format(str(i)))\n",
    "    \n",
    "    hist = model.fit_generator(gen.flow(x_train, y_train, batch_size=64),\n",
    "                        steps_per_epoch=(len(x_train)//64) + 1,\n",
    "                        validation_data=(x_valid,y_valid),\n",
    "                        validation_steps=(len(x_valid)//64)+1,\n",
    "                        epochs=50,\n",
    "                        verbose=2,\n",
    "                        callbacks=[early_stopping, model_checkpoint])\n",
    "    \n",
    "    tr_score = score_func(y_train, model.predict(x_train)[:, 0])\n",
    "    va_score = score_func(y_valid, model.predict(x_valid)[:, 0])\n",
    "    va_score_per_model[i] = va_score\n",
    "    \n",
    "    print('tr score: {}, va score: {}'.format(tr_score, va_score))\n",
    "    print('\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.96911011523687585,\n",
       " 1: 0.96760710553814011,\n",
       " 2: 0.97052648578811362,\n",
       " 3: 0.97368644427467954,\n",
       " 4: 0.97462249980235582,\n",
       " 5: 0.97120698876805134,\n",
       " 6: 0.96714783474365351,\n",
       " 7: 0.97741336633663356,\n",
       " 8: 0.97742200328407225,\n",
       " 9: 0.98473154362416104}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_score_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/yns207/data_invasive\n",
      "model: model_jun21_kfold_6.model\n",
      "tr score: 0.9761675887842398, va score: 0.98187267465895\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_0.model\n",
      "tr score: 0.968041864180145, va score: 0.9701529557668459\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_7.model\n",
      "tr score: 0.9741891735735538, va score: 0.9812319140140554\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_2.model\n",
      "tr score: 0.9708444074214698, va score: 0.9794336502687061\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_1.model\n",
      "tr score: 0.9722183774201617, va score: 0.977904092600248\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_9.model\n",
      "tr score: 0.9712507571438186, va score: 0.9769946258784622\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_5.model\n",
      "tr score: 0.9702094859802997, va score: 0.9751756924348904\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_3.model\n",
      "tr score: 0.9689396430979593, va score: 0.97474162877222\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_4.model\n",
      "tr score: 0.9743707611057283, va score: 0.981417941298057\n",
      "\n",
      "\n",
      "model: model_jun21_kfold_8.model\n",
      "tr score: 0.9715009161916396, va score: 0.977035965274907\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is actually using the final weights per epoch and we want the best weights for each epoch so\n",
    "# lets load them for each and calculate area under roc curve\n",
    "%cd $DATA_DIR\n",
    "g = glob.glob('model_jun21_kfold_*.model')\n",
    "\n",
    "model = make_model((128,128,3), grab_optimizer('sgd', 0.005))\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(train_img, train_label, test_size=0.20)\n",
    "\n",
    "for f in g:\n",
    "    model.load_weights(f)\n",
    "    tr_score = score_func(y_train, model.predict(x_train)[:, 0])\n",
    "    va_score = score_func(y_valid, model.predict(x_valid)[:, 0])\n",
    "    print('model: {}'.format(f))\n",
    "    print('tr score: {}, va score: {}'.format(tr_score, va_score))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "it looks like model 6 did the best here so we should try to use model 6 to mak ea predictions on the test set or maybe one of the other onef like model 4 or model 9 provide lower validation area under roc but will generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/yns207/data_invasive\n"
     ]
    }
   ],
   "source": [
    "# woth leaving a note:\n",
    "# prediction 1,2,3 from jun21 had misalgined the test names\n",
    "# and the predictions probably explains why my scores were so low\n",
    "# i realigned them now by no logner reading the file names from the flder\n",
    "# instaed using the test_set (because thats how the data was produced for\n",
    "# the test data)\n",
    "# also iaccidentally overwrote the 3rd submission so 3 and 4 are the same now...\n",
    "# ugh\n",
    "%cd $DATA_DIR\n",
    "batch_size=64\n",
    "model = make_model((128,128,3), grab_optimizer('sgd', 0.005))\n",
    "model.load_weights('model_jun21_kfold_6.model')\n",
    "preds = model.predict(test_img).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99625105,  0.11533546,  0.15051772,  0.10293393,  0.99481022,\n",
       "        0.10451093,  0.0870719 ,  0.99999034,  1.        ,  0.1845029 ], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1531, 2) (1531,)\n"
     ]
    }
   ],
   "source": [
    "print(test_set.shape, preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = test_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.996251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.115335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.102934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.994810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1  0.996251\n",
       "1     2  0.115335\n",
       "2     3  0.150518\n",
       "3     4  0.102934\n",
       "4     5  0.994810"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm['invasive'] = preds\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm.to_csv(os.path.join(DATA_DIR, 'results', 'subm_june_21_2017_4.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 332K\r\n",
      "drwxr-x--- 2 yns207 yns207 4,0K 21 juin  15:01 \u001b[0m\u001b[38;5;27m.\u001b[0m/\r\n",
      "drwxr-x--- 5 yns207 yns207 4,0K 21 juin  14:18 \u001b[38;5;27m..\u001b[0m/\r\n",
      "-rw-r----- 1 yns207 yns207  17K 21 juin  15:02 \u001b[38;5;9msubm_june_1_2017_2.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  19K 21 juin  15:02 \u001b[38;5;9msubm_june_1_2017_3.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  19K 21 juin  15:02 \u001b[38;5;9msubm_june_1_2017_4.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  19K 21 juin  15:02 \u001b[38;5;9msubm_june_1_2017_5.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  14K 21 juin  15:02 \u001b[38;5;9msubm_june_1_2017_6.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  17K 21 juin  15:02 \u001b[38;5;9msubm_june_1_2017.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207 3,4K 21 juin  15:02 \u001b[38;5;9msubm_june_21_2017_2.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  17K 21 juin  15:02 \u001b[38;5;9msubm_june_21_2017_3.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  17K 21 juin  15:02 \u001b[38;5;9msubm_june_21_2017_4.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  17K 21 juin  15:02 \u001b[38;5;9msubm_june_21_2017.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  14K 21 juin  15:02 \u001b[38;5;9msubm_june_2_2017.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207 3,4K 21 juin  15:02 \u001b[38;5;9msubm_june_8_2017_2.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  15K 21 juin  15:02 \u001b[38;5;9msubm_june_8_2017.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207 3,4K 21 juin  15:02 \u001b[38;5;9msubm_june_9_2017_2.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207 3,4K 21 juin  15:02 \u001b[38;5;9msubm_june_9_2017.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  18K 21 juin  15:02 \u001b[38;5;9msubm_may_27_2017_2.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  17K 21 juin  15:02 \u001b[38;5;9msubm_may_27_2017.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  18K 21 juin  15:02 \u001b[38;5;9msubm_may_29_2017_.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  18K 21 juin  15:02 \u001b[38;5;9msubm_may_30_2017_2.gz\u001b[0m\r\n",
      "-rw-r----- 1 yns207 yns207  18K 21 juin  15:02 \u001b[38;5;9msubm_may_30_2017.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!touch $DATA_DIR/results/*\n",
    "%ls -lah $DATA_DIR/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
