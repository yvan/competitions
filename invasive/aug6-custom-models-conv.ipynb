{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal of this notebook is to go back to what worked before (and did well on the learderboard) but fix batch normalization and input preprocessing. logically this should yield a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, bcolz, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, random_rotation, random_shear, random_zoom, random_shift, flip_axis\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_input_incep_xcep\n",
    "from keras.applications.imagenet_utils import preprocess_input as preprocess_input_vgg_resnet\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_optimizer(opt, lr):\n",
    "    if opt == 'sgd':\n",
    "        return optimizers.SGD(lr=lr, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    elif opt == 'adam':\n",
    "        return optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "def inception_block(filter_depth, inputs):\n",
    "    t1 = Convolution2D(filter_depth, (1, 1), padding='same', activation=None,use_bias=False)(inputs)\n",
    "    t1 = BatchNormalization()(t1)\n",
    "    t1 = Activation('relu')(t1)\n",
    "    \n",
    "    tower_1 = Convolution2D(filter_depth, (3, 3), padding='same', activation=None, use_bias=False)(t1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    \n",
    "    tower_2 = Convolution2D(filter_depth, (5, 5), padding='same', activation=None, use_bias=False)(t1)\n",
    "    tower_2 = BatchNormalization(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    \n",
    "    tower_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    tower_3 = Convolution2D(filter_depth, (1, 1), padding='same', activation=None, use_bias=False)(tower_3)\n",
    "    tower_3 = BatchNormalization()(tower_3)\n",
    "    tower_3 = Activation('relu')\n",
    "    \n",
    "    return concatenate([tower_1, tower_2, tower_3], axis=3)\n",
    "\n",
    "def conv_block(filter_depth, filter_size, pool_size, activation, drop_prob, inputs):\n",
    "    x = Convolution2D(filter_depth, filter_size, activation=None)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(units, activation, drop_prob, inputs):\n",
    "    x = Dense(units, activation=None)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    return x\n",
    "\n",
    "#my attempt at making a resnet identity block, wont be making any conv block\n",
    "def resnet_block(filter_depth, filter_size, pool_size, activation, inputs):\n",
    "    x = Convolution2D(filter_depth, (1,1), activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)\n",
    "\n",
    "    x = Convolution2D(filter_depth, filter_size, activation=None, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)\n",
    "    \n",
    "    x = Convolution2D(filter_depth, (1,1), activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    shortcut = Convolution2D(filter_depth, (1,1))(inputs)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = keras.layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def make_conv(input_shape, optimizer):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    m = conv_block(16, (3,3), (2,2),'relu', 0, inputs=inputs)\n",
    "    m = conv_block(32, (3,3), (2,2), 'relu', 0, inputs=m)\n",
    "    m = conv_block(64, (3,3), (2,2), 'relu', 0, inputs=m)\n",
    "    m = conv_block(128, (3,3), (2,2), 'relu', 0.25, inputs=m)\n",
    "    m = Flatten()(m)\n",
    "    m = dense_block(2048, 'relu', 0.55, inputs=m)\n",
    "    m = dense_block(512, 'relu', 0.65, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_incep(input_shape, optimizer):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # convolution preproccesing\n",
    "    m = BatchNormalization()(inputs)\n",
    "    m = Convolution2D(16, (3, 3), strides=(2,2), padding='valid', activation='relu')(m)\n",
    "    m = MaxPooling2D((3, 3), strides=(2, 2))(m)\n",
    "    #inception blocks\n",
    "    m = BatchNormalization()(m)\n",
    "    m = inception_block(32, m)\n",
    "    m = inception_block(64, m)\n",
    "    m = inception_block(128, m)\n",
    "    m = inception_block(256, m)\n",
    "    m = GlobalAveragePooling2D()(m)\n",
    "    m = BatchNormalization()(m)\n",
    "    m = dense_block(1024, 'relu', 0.25, inputs=m)\n",
    "    m = dense_block(512, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_resnet(input_shape, optimizer):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    m = ZeroPadding2D((3,3))(inputs)\n",
    "    m = Convolution2D(16, (3,3), strides=(2,2), activation='relu')(m)\n",
    "    m = MaxPooling2D((3,3), strides=(2,2))(m)\n",
    "    m = resnet_block(32, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = resnet_block(64, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = resnet_block(128, (3,3), (2,2), 'relu', inputs=m)\n",
    "    m = AveragePooling2D((7, 7))(m)\n",
    "    m = Flatten()(m)\n",
    "    m = dense_block(2048, 'relu', 0.25, inputs=m)\n",
    "    m = dense_block(512, 'relu', 0.5, inputs=m)\n",
    "    outputs = dense_block(1, 'sigmoid', 0, inputs=m)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# does not use precomputiation so it can use data augmentation\n",
    "def train_kfolds(model, train_data, train_label, augment, model_out, model_init_weights, epochs, kfolds, batch_size):\n",
    "    kf = KFold(n_splits=kfolds, shuffle=True)\n",
    "    \n",
    "    i = 0\n",
    "    models_stats = {}\n",
    "    for train_ixs, valid_ixs in kf.split(train_data):\n",
    "        x_train = train_data[train_ixs]\n",
    "        x_valid = train_data[valid_ixs]\n",
    "        y_train = train_label[train_ixs]\n",
    "        y_valid = train_label[valid_ixs]\n",
    "        \n",
    "        #augment the data\n",
    "        if augment:\n",
    "            for i in range(len(x_train)):\n",
    "                x_train[i] = random_rotation(x_train[i], 20) \n",
    "                x_train[i] = random_shear(x_train[i], 0.2)\n",
    "                x_train[i] = random_zoom(x_train[i], [0.8, 1.2])\n",
    "                x_train[i] = random_shift(x_train[i], 0.2, 0.2)\n",
    "        \n",
    "        #re-initialzie the weights of the model on each run\n",
    "        #by loading thi intiial stored weights from file\n",
    "        model = load_model(model_init_weights)\n",
    "        model_out_file = '{}_{}.model'.format(model_out, str(i))\n",
    "        model_checkpoint = ModelCheckpoint(model_out_file, \n",
    "                                            monitor='val_loss', \n",
    "                                            save_best_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_acc',\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              factor=0.1,\n",
    "                              cooldown=10,\n",
    "                              min_lr=0.00001)\n",
    "        \n",
    "        model.fit(x=x_train, y=y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      validation_data=(x_valid,y_valid),\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      callbacks=[model_checkpoint, reduce_lr])\n",
    "        \n",
    "        model = load_model(model_out_file)\n",
    "        eval_tr = model.evaluate(x_train, y_train)\n",
    "        eval_va = model.evaluate(x_valid, y_valid)\n",
    "        tr_score = roc_auc_score(y_train, model.predict(x_train)[:, 0])\n",
    "        va_score = roc_auc_score(y_valid, model.predict(x_valid)[:, 0])\n",
    "        \n",
    "        print('\\n')\n",
    "        print('kfold: {}'.format(str(i)))\n",
    "        print('best model train acc: {}, loss: {}'.format(eval_tr[1], eval_tr[0]))\n",
    "        print('best model valid acc: {}, loss: {}'.format(eval_va[1], eval_va[0]))\n",
    "        print('best model train aroc score: {}, valid aroc score: {}'.format(tr_score, va_score))\n",
    "        print('\\n')\n",
    "        models_stats[model_out_file] = {'score_tr_va':[tr_score, va_score], 'train_acc_loss':[eval_tr[1], eval_tr[0]], 'val_acc_loss':[eval_va[1], eval_va[0]]}\n",
    "        \n",
    "        with open(os.path.join(models_path,'{}_{}.out'.format(model_out,'history')), 'a') as f:\n",
    "            f.write('kfold: {}'.format(str(i)))\n",
    "            f.write('best model train acc: {}, loss: {}'.format(eval_tr[1], eval_tr[0]))\n",
    "            f.write('best model valid acc: {}, loss: {}'.format(eval_va[1], eval_va[0]))\n",
    "            f.write('best model train aroc score: {}, valid aroc score: {}'.format(tr_score, va_score))\n",
    "            f.write('\\n')\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return models_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.path.join('/scratch', 'yns207', 'data_invasive')\n",
    "test_path = os.path.join(path, 'test')\n",
    "reults_path = os.path.join(path, 'results')\n",
    "train_path = os.path.join(path, 'train')\n",
    "valid_path = os.path.join(path, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n",
    "test_set = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
    "\n",
    "def read_img(img_path, img_shape):\n",
    "    img = misc.imread(img_path)\n",
    "    img = misc.imresize(img, img_shape)\n",
    "    return img\n",
    "\n",
    "def read_imgs(img_height, img_width):\n",
    "    train_img, test_img = [],[]\n",
    "    for img_path in tqdm(train_set['name'].iloc[:]):\n",
    "        train_img.append(read_img(os.path.join(path, 'train', str(img_path)+'.jpg'), (img_height, img_width)))\n",
    "\n",
    "    for img_path in tqdm(test_set['name'].iloc[:]):\n",
    "        test_img.append(read_img(os.path.join(path, 'test', str(img_path)+'.jpg'), (img_height, img_width)))\n",
    "    return np.array(train_img), np.array(test_img)\n",
    "\n",
    "train_img, test_img = read_imgs(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.array(train_set['invasive'].iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $path\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "kfolds = 5\n",
    "lr = 0.001\n",
    "\n",
    "model_name = 'invasive_customconv_aug7'\n",
    "init_weights_model = '{}_base.model'.format(model_name)\n",
    "\n",
    "model = make_conv(train_img[0].shape, grab_optimizer('adam', lr))\n",
    "model.save(init_weights_model)\n",
    "\n",
    "# train dense model on folds\n",
    "performance = train_kfolds(model, train_img, train_labels, False, model_name, init_weights_model, epochs, kfolds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
